{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462a7a5d638814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = '.'\n",
    "# If using google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    dir_root = '/content/drive/MyDrive/Colab Notebooks/ESE546/hw3'\n",
    "\n",
    "print(dir_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b430103d02bbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "302b430103d02bbe",
    "outputId": "baaf358c-4e61-453a-b1f6-262d1a218350"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load the text from a local file\n",
    "def load_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Load the text from a URL\n",
    "def load_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    text = response.text.replace('\\r\\n', '\\n')  # Normalize line endings\n",
    "    return text\n",
    "\n",
    "# Count unique characters in the text\n",
    "def count_unique_chars(text):\n",
    "    unique_chars = set(text)\n",
    "    # Num of unique characters\n",
    "    vocab_size = len(unique_chars)\n",
    "    return vocab_size, unique_chars\n",
    "\n",
    "# List of file paths or URLs\n",
    "local_file_1 = 'pg100.txt'\n",
    "local_file_2 = 'pg2600.txt'\n",
    "local_file_3 = 'pg766.txt'\n",
    "url_file_1 = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "url_file_2 = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "url_file_3 = 'https://www.gutenberg.org/cache/epub/766/pg766.txt'\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    file_path_list = [url_file_1, url_file_2, url_file_3]\n",
    "else:\n",
    "    file_path_list = [local_file_1, local_file_2, local_file_3]\n",
    "text_list = []\n",
    "vocab_size_list = []\n",
    "unique_chars_list = []\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    if file_path.startswith('http'):\n",
    "        print(f'Loading text from URL: {file_path}')\n",
    "        text = load_text_from_url(file_path)\n",
    "    else:\n",
    "        print(f'Loading text from file: {file_path}')\n",
    "        text = load_text_from_file(file_path)\n",
    "    vocab_size, unique_chars = count_unique_chars(text)\n",
    "    text_list.append(text)\n",
    "    vocab_size_list.append(vocab_size)\n",
    "    unique_chars_list.append(unique_chars)\n",
    "\n",
    "print(f'Vocabulary size for each text: {vocab_size_list}')\n",
    "print(f'Unique characters for each text: {unique_chars_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29800fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_list[0][:100])\n",
    "print(text_list[1][:100])\n",
    "print(text_list[2][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1ba040297cb3e10",
   "metadata": {
    "id": "b1ba040297cb3e10"
   },
   "outputs": [],
   "source": [
    "# # Compare unique characters from local and URL text files\n",
    "# local_file_path = 'pg2600.txt'\n",
    "# url_file_path = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "#\n",
    "# local_text = load_text_from_file(local_file_path)\n",
    "# local_vocab_size, local_unique_chars = count_unique_chars(local_text)\n",
    "# url_text = load_text_from_url(url_file_path)\n",
    "# url_vocab_size, url_unique_chars = count_unique_chars(url_text)\n",
    "#\n",
    "# # Find the extra character(s)\n",
    "# extra_chars_in_url = url_unique_chars - local_unique_chars\n",
    "# extra_chars_in_local = local_unique_chars - url_unique_chars\n",
    "# print(f'Extra characters in URL text: {extra_chars_in_url}')\n",
    "# print(f'Extra characters in local text: {extra_chars_in_local}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e595766dd8deb1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e595766dd8deb1b",
    "outputId": "e4bba092-0ed1-4fb8-f5bb-712821c6766a"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map characters to indices and vice-versa\n",
    "def create_char_mappings(unique_chars):\n",
    "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    index_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "vocab_size, unique_chars = vocab_size_list[0], unique_chars_list[0]\n",
    "char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "\n",
    "print(f\"Character to index mapping for first text: {char_to_index}\")\n",
    "print(f\"Index to character mapping for first text: {index_to_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff240b4b6efa2ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cff240b4b6efa2ea",
    "outputId": "6b2b808a-d632-4e34-e632-cadd9f71be6e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# One-hot encode a character based on the character index\n",
    "def one_hot_encode(char, char_to_index, vocab_size):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[char_to_index[char]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "test_char_a = 'a'\n",
    "one_hot_vector = one_hot_encode(test_char_a, char_to_index, vocab_size)\n",
    "print(f\"One-hot encoding for '{test_char_a}': {one_hot_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac26dff8806f1e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ac26dff8806f1e0",
    "outputId": "2b139668-1682-45d1-aaf9-029a8f729b6f"
   },
   "outputs": [],
   "source": [
    "# Sample a short portion from the first book\n",
    "sample_text_500 = text_list[0][60000:60500]\n",
    "print(sample_text_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da41e402067fbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# # Generate sequences of 32 characters and the next character as the target\n",
    "# def generate_sequences(text, char_to_index, sequence_length=32, stride=1):\n",
    "#     input_sequences = []\n",
    "#     target_characters = []\n",
    "\n",
    "#     # for i in range(0, len(text) - sequence_length, stride):\n",
    "#     for i in tqdm(range(0, len(text) - sequence_length - 1, stride), desc=\"Generating sequences\"):\n",
    "#         input_seq = text[i:i+sequence_length]\n",
    "#         # Target/next character\n",
    "#         target_char = text[i+sequence_length]\n",
    "\n",
    "#         # Convert input sequence to one-hot encoded vectors\n",
    "#         input_seq_encoded = [one_hot_encode(c, char_to_index, len(char_to_index)) for c in input_seq]\n",
    "#         input_sequences.append(input_seq_encoded)\n",
    "\n",
    "#         # One-hot encoding for the target character\n",
    "#         target_char_encoded = one_hot_encode(target_char, char_to_index, len(char_to_index))\n",
    "#         target_characters.append(target_char_encoded)\n",
    "\n",
    "#     return np.array(input_sequences), np.array(target_characters)\n",
    "\n",
    "# Generate sequences of 32 characters and the next 32 after stride length as the target\n",
    "def generate_sequences(\n",
    "    text, char_to_index, input_seq_len=32, target_seq_len=32, stride=16, target_offset=1\n",
    "):\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "\n",
    "    prevent_index_overflow = max(input_seq_len, target_seq_len + target_offset)\n",
    "\n",
    "    for i in tqdm(range(0, len(text) - prevent_index_overflow, stride), desc=\"Generating sequences\"):\n",
    "        input_seq = text[i : i + input_seq_len]\n",
    "        target_seq = text[i + target_offset : i + target_seq_len + target_offset]\n",
    "\n",
    "        # Convert to one-hot encoded vectors\n",
    "        input_seq_encoded = [\n",
    "            one_hot_encode(c, char_to_index, len(char_to_index)) for c in input_seq\n",
    "        ]\n",
    "        target_seq_encoded = [\n",
    "            one_hot_encode(c, char_to_index, len(char_to_index)) for c in target_seq\n",
    "        ]\n",
    "\n",
    "        input_sequences.append(input_seq_encoded)\n",
    "        target_sequences.append(target_seq_encoded)\n",
    "\n",
    "    return np.array(input_sequences), np.array(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61dfa3f256a14a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b61dfa3f256a14a8",
    "outputId": "b88f80aa-823f-48d0-b5d4-3322bb313126"
   },
   "outputs": [],
   "source": [
    "input_seqs, target_seqs = generate_sequences(sample_text_500, char_to_index, input_seq_len=32, target_seq_len=32, stride=2)\n",
    "print(f\"Input sequences shape: {input_seqs.shape}\")\n",
    "print(f\"Target characters shape: {target_seqs.shape}\")\n",
    "\n",
    "# randomly pick a sample input and target sequence\n",
    "idx = np.random.randint(0, input_seqs.shape[0])\n",
    "sample_input_seq = input_seqs[idx]\n",
    "sample_target_seq = target_seqs[idx]\n",
    "print(sample_input_seq)\n",
    "print(sample_target_seq)\n",
    "# print the sequence mapped to characters\n",
    "print(\">Sample input sequence:\")\n",
    "print(''.join([index_to_char[np.argmax(char)] for char in sample_input_seq]))\n",
    "print(\">Sample target sequence:\")\n",
    "print(''.join([index_to_char[np.argmax(char)] for char in sample_target_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61aff6c6ca7909",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b61aff6c6ca7909",
    "outputId": "c9a4ddaa-39c9-4fb7-cf16-2b90e015c042"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, rnn_layers=1, device=device, dropout=0.2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.rnn = nn.RNN(self.input_size, self.hidden_size, num_layers=self.rnn_layers, nonlinearity='tanh', batch_first=True)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # batch_size = x.size(0)\n",
    "        # hidden = self.init_hidden(batch_size)\n",
    "        # hidden = hidden.to(self.device)\n",
    "\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous()#.view(-1, self.hidden_size)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        # return F.log_softmax(out, dim=1), hidden\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.rnn_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "def repackage_hidden(hidden_layer):\n",
    "    # Detach hidden states from their history\n",
    "    if isinstance(hidden_layer, torch.Tensor):\n",
    "        return hidden_layer.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2895b85ad94231",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ALL_BOOKS = False\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    USE_ALL_BOOKS = True\n",
    "\n",
    "sequence_length = 32\n",
    "stride = 24\n",
    "target_offset = 1\n",
    "\n",
    "if USE_ALL_BOOKS:\n",
    "    # Load text and prepare data\n",
    "    all_texts = ''.join(text_list)  # Concatenate all texts\n",
    "    # Pick the first 1/30, mid 1/30, and last 1/30 for training\n",
    "    all_texts = all_texts[:len(all_texts)//30]\n",
    "    all_texts += all_texts[len(all_texts)//2:len(all_texts)//2 + len(all_texts)//30]\n",
    "    all_texts += all_texts[len(all_texts)//2 + len(all_texts)//30:]\n",
    "    vocab_size, unique_chars = count_unique_chars(all_texts)\n",
    "    char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    input_seqs, target_seqs = generate_sequences(all_texts, char_to_index, input_seq_len=sequence_length, target_seq_len=sequence_length, stride=stride, target_offset=target_offset)\n",
    "else:\n",
    "    text = text_list[0]\n",
    "    # sample only the first 1/100 of the text\n",
    "    text = text[:len(text)//100]\n",
    "    vocab_size, unique_chars = count_unique_chars(text)\n",
    "    char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    input_seqs, target_seqs = generate_sequences(text, char_to_index, input_seq_len=sequence_length, target_seq_len=sequence_length, stride=stride, target_offset=target_offset)\n",
    "\n",
    "input_seqs = torch.tensor(input_seqs, dtype=torch.float32).to(device)\n",
    "target_seqs = torch.tensor(target_seqs, dtype=torch.long).to(device)\n",
    "    \n",
    "dataset = TensorDataset(input_seqs, target_seqs)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = TensorDataset(input_seqs[:train_size], target_seqs[:train_size])\n",
    "val_dataset = TensorDataset(input_seqs[train_size:], target_seqs[train_size:])\n",
    "    \n",
    "print(\"Input seqs shape:\", input_seqs.shape)\n",
    "print(\"Target seqs shape:\", target_seqs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600385dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_sequence(seq):\n",
    "    return seq.replace(' ', '\\\\s').replace('\\n', '\\\\n')\n",
    "\n",
    "# Randomly pick a sequence and target sequence\n",
    "# convert them back to characters\n",
    "# and check if the conversion is correct\n",
    "sequence_idx = np.random.randint(0, len(input_seqs))\n",
    "input_seq = input_seqs[sequence_idx]\n",
    "target_seq = target_seqs[sequence_idx]\n",
    "\n",
    "input_indices = torch.argmax(input_seq, dim=1).cpu().numpy()\n",
    "input_chars = [index_to_char[idx] for idx in input_indices]\n",
    "target_indices = torch.argmax(target_seq, dim=1).cpu().numpy()\n",
    "target_chars = [index_to_char[idx] for idx in target_indices]\n",
    "\n",
    "input_str = ''.join(input_chars)\n",
    "target_str = ''.join(target_chars)\n",
    "escaped_input_str = escape_sequence(input_str)\n",
    "escaped_target_str = escape_sequence(target_str)\n",
    "\n",
    "print(f\">Input seq: \\n{escaped_input_str}\")\n",
    "print(f\">Target seq: \\n{escaped_target_str}\")\n",
    "\n",
    "# Also find the sequence in the original txt ebooks\n",
    "# and print the entire sequence with the next target character\n",
    "for book_idx, text in enumerate(text_list):\n",
    "    input_start_indices = [i for i in range(len(text)) if text.startswith(input_str, i)]\n",
    "    for input_start_idx in input_start_indices:\n",
    "        # Make sure the sequence is not at the end of the text\n",
    "        if input_start_idx + len(input_str) + len(target_str) < len(text):\n",
    "            # Extract the sequence and the next character\n",
    "            excerpt = text[input_start_idx:input_start_idx + max(len(input_str), len(target_str) + stride)]\n",
    "            escaped_excerpt = escape_sequence(excerpt)\n",
    "            print(f\">Found the sequence in book {book_idx + 1}, original complete sequence:\")\n",
    "            print(f\"{escaped_excerpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3b38fd80144a4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "4c3b38fd80144a4c",
    "outputId": "bc040813-dedc-4488-9e61-21e68017c642"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 512\n",
    "learning_rate = 0.001\n",
    "dropout = 0.5\n",
    "num_epochs = 100 if 'google.colab' in str(get_ipython()) else 20\n",
    "batch_size = 64\n",
    "rnn_layers = 1\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, BatchSampler\n",
    "\n",
    "# Create samplers that allow replacement to ensure each batch has the same size\n",
    "train_sampler = RandomSampler(train_dataset, replacement=True, num_samples=len(train_dataset))\n",
    "val_sampler = RandomSampler(val_dataset, replacement=True, num_samples=len(val_dataset))\n",
    "\n",
    "# Create batch samplers with the same batch size and drop the last batch\n",
    "train_batch_sampler = BatchSampler(train_sampler, batch_size=batch_size, drop_last=True)\n",
    "val_batch_sampler = BatchSampler(val_sampler, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_sampler=train_batch_sampler)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_sampler=val_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52439b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = CharRNN(vocab_size, vocab_size, hidden_size, rnn_layers, device=device, dropout=dropout).to(device)\n",
    "# criterion = nn.NLLLoss().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)  # Adjust gamma as needed\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "update_counts = []\n",
    "update_cnt = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size).to(device)\n",
    "    for input_seqs, target_seqs in train_loader:\n",
    "        model.train()\n",
    "        batch_size = input_seqs.size(0)\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(input_seqs, hidden)\n",
    "        loss = criterion(output.transpose(1, 2), torch.argmax(target_seqs, dim=2))\n",
    "        loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Record training loss and accuracy every 100 updates\n",
    "        if update_cnt % 100 == 0:\n",
    "            training_losses.append(loss.item())\n",
    "            update_counts.append(update_cnt)\n",
    "\n",
    "            training_accuracy = (torch.argmax(output[:, -1, :], dim=1) == torch.argmax(target_seqs[:, -1, :], dim=1)).float().mean().item()\n",
    "            training_accuracies.append(training_accuracy)\n",
    "\n",
    "        if update_cnt % 1000 == 0:\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                correct_predictions = 0\n",
    "                total_predictions = 0\n",
    "                for input_seqs, target_seqs in val_loader:\n",
    "                    batch_size = input_seqs.size(0)\n",
    "                    input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "                    hidden = model.init_hidden(batch_size).to(device)\n",
    "                    hidden = repackage_hidden(hidden)\n",
    "                    output, hidden = model(input_seqs, hidden)\n",
    "                    val_loss += criterion(output.transpose(1, 2), torch.argmax(target_seqs, dim=2)).item()\n",
    "                    \n",
    "                    correct_predictions += (torch.argmax(output[:, -1, :], dim=1) == torch.argmax(target_seqs[:, -1, :], dim=1)).sum().item()\n",
    "                    total_predictions += target_seqs.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            validation_accuracy = correct_predictions / total_predictions\n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "            print(f'Update {update_cnt}, Training Loss: {loss}, Training Accuracy: {training_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}')\n",
    "\n",
    "        update_cnt += 1\n",
    "    \n",
    "    scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_input = np.array([73, 62, 66, 67, 20, 41, 58, 51, 61, 73, 56, 66, 67, 62, 66, 67, 61, 73, 61, 67, 64, 72,  8, 37, 62, 52, 72, 45, 73, 40, 67, 73])\n",
    "temp_test_target = np.array([62, 66, 67, 20, 41, 58, 51, 61, 73, 56, 66, 67, 62, 66, 67, 61, 73, 61, 67, 64, 72,  8, 37, 62, 52, 72, 45, 73, 40, 67, 73, 62])\n",
    "temp_test_input_char = []\n",
    "temp_test_target_char = []\n",
    "for i in range(len(temp_test_input)):\n",
    "  temp_test_input_char.append(index_to_char[temp_test_input[i]])\n",
    "for i in range(len(temp_test_target)):\n",
    "  temp_test_target_char.append(index_to_char[temp_test_target[i]])\n",
    "print(\">Input:\")\n",
    "print(''.join(temp_test_input_char))\n",
    "print(\">Target:\")\n",
    "print(''.join(temp_test_target_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save the accuracies and errors and .npy file\n",
    "np.save(f'{dir_root}/training_losses.npy', training_losses)\n",
    "np.save(f'{dir_root}/validation_losses.npy', validation_losses)\n",
    "np.save(f'{dir_root}/training_accuracies.npy', training_accuracies)\n",
    "np.save(f'{dir_root}/validation_accuracies.npy', validation_accuracies)\n",
    "np.save(f'{dir_root}/update_counts.npy', update_counts)\n",
    "\n",
    "# Calculate errors: error = 1 - accuracy\n",
    "training_errors = [1 - acc for acc in training_accuracies]\n",
    "validation_errors = [1 - acc for acc in validation_accuracies]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(update_counts, training_losses, label='Training Loss')\n",
    "plt.plot(range(0, len(validation_losses) * 1000, 1000), validation_losses, label='Validation Loss') \n",
    "plt.xlabel('Weight Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss vs. Weight Updates')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(update_counts, training_errors, label='Training Error')\n",
    "plt.plot(range(0, len(validation_errors) * 1000, 1000), validation_errors, label='Validation Error')  \n",
    "plt.xlabel('Weight Updates')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training and Validation Error vs. Weight Updates')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c109614febf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = f'{dir_root}/char_rnn_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e976a5ab339735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = f'{dir_root}/char_rnn_model.pth'\n",
    "model = CharRNN(vocab_size, vocab_size, hidden_size, rnn_layers, device=device, dropout=dropout).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e5c69371b00f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, char_to_index, index_to_char, max_length=1000, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(1).to(device)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        input_seq = start_text\n",
    "        generated_text = start_text\n",
    "\n",
    "        for i in range(max_length):\n",
    "            input_seq_encoded = torch.tensor([one_hot_encode(c, char_to_index, len(char_to_index)) for c in input_seq], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            output, hidden = model(input_seq_encoded, hidden)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            \n",
    "            # Ensure the predicted index is within the valid range\n",
    "            if top_char.item() in index_to_char:\n",
    "                predicted_char = index_to_char[top_char.item()]\n",
    "            else:\n",
    "                predicted_char = ''  # Handle out-of-range index by skipping or using a placeholder\n",
    "\n",
    "            generated_text += predicted_char\n",
    "            input_seq = input_seq[1:] + predicted_char\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720fb045661f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start texts and temperatures\n",
    "start_texts = [\n",
    "    \"I never shall forget th\",\n",
    "    \"Man! What can I say? Mamb\",\n",
    "    \"Will our path converge beneat\",\n",
    "    \"It has been a long day, wi\"\n",
    "]\n",
    "\n",
    "temperatures = [0.2, 0.5, 0.7, 1.0]\n",
    "\n",
    "# Loop through start texts and temperatures\n",
    "for i, (start_text, temperature) in enumerate(zip(start_texts, temperatures), 1):\n",
    "    generated_text = generate_text(model, start_text, char_to_index, index_to_char, max_length=100, temperature=temperature)\n",
    "    print(f\">Generating test {i}:\\n\", generated_text)\n",
    "    # Save the generated text to a file\n",
    "    with open(f'{dir_root}/generated_text_rnn_{i}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down if it's google colab\n",
    "# First sleep for a while so that changes to the notebook are saved\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
