{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462a7a5d638814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_root = '.'\n",
    "# If using google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    dir_root = '/content/drive/MyDrive/Colab Notebooks/ESE546/hw3'\n",
    "\n",
    "print(dir_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b430103d02bbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "302b430103d02bbe",
    "outputId": "baaf358c-4e61-453a-b1f6-262d1a218350"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load the text from a local file\n",
    "def load_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Load the text from a URL\n",
    "def load_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    text = response.text.replace('\\r\\n', '\\n')  # Normalize line endings\n",
    "    return text\n",
    "\n",
    "# Count unique characters in the text\n",
    "def count_unique_chars(text):\n",
    "    unique_chars = set(text)\n",
    "    # Num of unique characters\n",
    "    vocab_size = len(unique_chars)\n",
    "    return vocab_size, unique_chars\n",
    "\n",
    "# List of file paths or URLs\n",
    "local_file_1 = 'pg100.txt'\n",
    "local_file_2 = 'pg2600.txt'\n",
    "local_file_3 = 'pg766.txt'\n",
    "url_file_1 = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "url_file_2 = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "url_file_3 = 'https://www.gutenberg.org/cache/epub/766/pg766.txt'\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    file_path_list = [url_file_1, url_file_2, url_file_3]\n",
    "else:\n",
    "    file_path_list = [local_file_1, local_file_2, local_file_3]\n",
    "text_list = []\n",
    "vocab_size_list = []\n",
    "unique_chars_list = []\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    if file_path.startswith('http'):\n",
    "        print(f'Loading text from URL: {file_path}')\n",
    "        text = load_text_from_url(file_path)\n",
    "    else:\n",
    "        print(f'Loading text from file: {file_path}')\n",
    "        text = load_text_from_file(file_path)\n",
    "    vocab_size, unique_chars = count_unique_chars(text)\n",
    "    text_list.append(text)\n",
    "    vocab_size_list.append(vocab_size)\n",
    "    unique_chars_list.append(unique_chars)\n",
    "\n",
    "print(f'Vocabulary size for each text: {vocab_size_list}')\n",
    "print(f'Unique characters for each text: {unique_chars_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map characters to indices and vice-versa\n",
    "def create_char_mappings(unique_chars):\n",
    "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    index_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "vocab_size, unique_chars = vocab_size_list[0], unique_chars_list[0]\n",
    "char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "print(f\"Character to index mapping for first text: {char_to_index}\")\n",
    "print(f\"Index to character mapping for first text: {index_to_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64b0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def generate_sequences_for_transformer(text, char_to_index, seq_length=128, stride=16, target_offset=1):\n",
    "    \"\"\"\n",
    "    Generate sequences of indices for transformer training.\n",
    "    Returns input and target sequences as integer indices instead of one-hot vectors.\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(text) - seq_length - target_offset, stride), \n",
    "                 desc=\"Generating sequences\"):\n",
    "        # Input sequence\n",
    "        input_seq = text[i:i + seq_length]\n",
    "        # Target sequence (shifted by 1 position)\n",
    "        target_seq = text[i + target_offset:i + target_offset + seq_length]\n",
    "        \n",
    "        # Convert characters to indices\n",
    "        input_indices = [char_to_index[c] for c in input_seq]\n",
    "        target_indices = [char_to_index[c] for c in target_seq]\n",
    "        \n",
    "        input_sequences.append(input_indices)\n",
    "        target_sequences.append(target_indices)\n",
    "    \n",
    "    return np.array(input_sequences), np.array(target_sequences)\n",
    "\n",
    "def generate_sequences_for_transformer_(text, char_to_index, seq_length=128, stride=16, target_offset=1):\n",
    "    \"\"\"\n",
    "    Generate sequences where:\n",
    "    - input_sequences are characters 1-128\n",
    "    - target_sequences are characters 129-256\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "    target_sequences = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(text) - seq_length*2, stride)):\n",
    "        # Input sequence: first 128 characters\n",
    "        input_seq = text[i:i + seq_length]\n",
    "        # Target sequence: next 128 characters\n",
    "        target_seq = text[i + seq_length:i + seq_length*2]\n",
    "        \n",
    "        input_indices = [char_to_index[c] for c in input_seq]\n",
    "        target_indices = [char_to_index[c] for c in target_seq]\n",
    "        \n",
    "        input_sequences.append(input_indices)\n",
    "        target_sequences.append(target_indices)\n",
    "    \n",
    "    return np.array(input_sequences), np.array(target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ff0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "USE_ALL_BOOKS = False\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    USE_ALL_BOOKS = True\n",
    "\n",
    "sequence_length = 128\n",
    "stride = 64\n",
    "target_offset = 1\n",
    "\n",
    "if USE_ALL_BOOKS:\n",
    "    # Load text and prepare data\n",
    "    all_texts = ''.join(text_list)\n",
    "    # all_texts = all_texts[:len(all_texts)//10]\n",
    "    # all_texts += all_texts[len(all_texts)//2:len(all_texts)//2 + len(all_texts)//10]\n",
    "    # all_texts += all_texts[len(all_texts)//2 + len(all_texts)//10:]\n",
    "    vocab_size, unique_chars = count_unique_chars(all_texts)\n",
    "    char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    input_seqs, target_seqs = generate_sequences_for_transformer(\n",
    "        all_texts, char_to_index, seq_length=sequence_length, stride=stride, target_offset=target_offset\n",
    "    )\n",
    "else:\n",
    "    text = text_list[0]\n",
    "    text = text[:len(text)//1]\n",
    "    vocab_size, unique_chars = count_unique_chars(text)\n",
    "    char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    input_seqs, target_seqs = generate_sequences_for_transformer(\n",
    "        text, char_to_index, seq_length=sequence_length, stride=stride, target_offset=target_offset\n",
    "    )\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_seqs = torch.tensor(input_seqs, dtype=torch.long).to(device)\n",
    "target_seqs = torch.tensor(target_seqs, dtype=torch.long).to(device)\n",
    "\n",
    "# Create datasets\n",
    "dataset = TensorDataset(input_seqs, target_seqs)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset = TensorDataset(input_seqs[:train_size], target_seqs[:train_size])\n",
    "val_dataset = TensorDataset(input_seqs[train_size:], target_seqs[train_size:])\n",
    "\n",
    "print(\"Input sequences shape:\", input_seqs.shape)\n",
    "print(\"Target sequences shape:\", target_seqs.shape)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb274a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_input_seq_0 = input_seqs[0].cpu().numpy()\n",
    "temp_test_target_seq_0 = target_seqs[0].cpu().numpy()\n",
    "print(\">Example input sequence:\\n\", ''.join([index_to_char[idx] for idx in temp_test_input_seq_0]))\n",
    "print(\">Example target sequence:\\n\", ''.join([index_to_char[idx] for idx in temp_test_target_seq_0]))\n",
    "temp_test_input_seq_1 = input_seqs[1].cpu().numpy()\n",
    "temp_test_target_seq_1 = target_seqs[1].cpu().numpy()\n",
    "print(\">Example input sequence:\\n\", ''.join([index_to_char[idx] for idx in temp_test_input_seq_1]))\n",
    "print(\">Example target sequence:\\n\", ''.join([index_to_char[idx] for idx in temp_test_target_seq_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42f7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, \n",
    "                 dim_feedforward=1024, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.decoder = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        mask = self._generate_square_subsequent_mask(src.size(1)).to(src.device)\n",
    "        src_mask = mask\n",
    "        \n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "class WarmupLR:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.d_model ** (-0.5) * min(self.step_num ** (-0.5), \n",
    "                                        self.step_num * self.warmup_steps ** (-1.5))\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "model = TransformerModel(\n",
    "    vocab_size,\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_layers=6,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = WarmupLR(optimizer, d_model=512)\n",
    "\n",
    "# Training and validation tracking\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "update_counts = []\n",
    "update_cnt = 0\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (input_seq, target_seq) in enumerate(train_loader):\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        seq_len = input_seq.size(1)\n",
    "        # src_mask = generate_square_subsequent_mask(seq_len).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Record metrics every 100 updates\n",
    "        if update_cnt % 100 == 0:\n",
    "            training_losses.append(loss.item())\n",
    "            update_counts.append(update_cnt)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = output.argmax(dim=-1)\n",
    "                accuracy = (pred == target_seq).float().mean().item()\n",
    "                training_accuracies.append(accuracy)\n",
    "        \n",
    "        # Validation every 1000 updates\n",
    "        if update_cnt % 1000 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_accuracy = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for val_input, val_target in val_loader:\n",
    "                    val_input, val_target = val_input.to(device), val_target.to(device)\n",
    "                    # val_mask = generate_square_subsequent_mask(seq_len).to(device)\n",
    "\n",
    "                    # Predict next 128 characters\n",
    "                    pred_output = model(val_input)\n",
    "\n",
    "                    # Calculate loss on the future sequence\n",
    "                    val_loss += criterion(pred_output.view(-1, vocab_size), val_target.view(-1)).item()\n",
    "                    \n",
    "                    # For accuracy\n",
    "                    val_pred = pred_output.argmax(dim=-1)\n",
    "                    val_accuracy += (val_pred == val_target).float().mean().item()\n",
    "                    num_batches += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / num_batches\n",
    "            avg_val_accuracy = val_accuracy / num_batches\n",
    "            validation_losses.append(avg_val_loss)\n",
    "            validation_accuracies.append(avg_val_accuracy)\n",
    "            \n",
    "            print(f'Update {update_cnt}, Train Loss: {loss.item():.4f}, '\n",
    "                  f'Train Acc: {accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, '\n",
    "                  f'Val Acc: {avg_val_accuracy:.4f}, '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.4f}')\n",
    "            \n",
    "            model.train()\n",
    "        \n",
    "        update_cnt += 1\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Average loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_input = np.array([58,  9, 41, 55, 27, 93, 41,  9, 68,  6, 55,  6,  1, 72, 66, 66, 82, 41,\n",
    "        55, 73, 77, 30, 78, 72, 55, 62, 14, 72, 55, 77, 73,  9, 83,  6, 48, 55,\n",
    "        30, 31, 55, 43, 30, 31, 64, 74, 14, 72, 55, 10, 31, 72,  6,  6, 55, 30,\n",
    "        43, 55, 77, 30, 73, 62,  6, 55, 30, 31, 55, 10, 31, 79, 41, 72, 21, 55,\n",
    "        58, 55, 10, 79, 72, 75, 72, 55, 30, 43, 55,  1, 30, 31, 83, 64, 45, 30,\n",
    "        55, 77, 31, 73, 78, 72, 66, 93, 55, 41, 30,  9, 72, 48, 55,  6, 30, 55,\n",
    "        31, 79, 75, 14, 48, 55, 62, 14, 73, 62, 55, 79, 62, 55, 41, 79, 41, 55,\n",
    "         6, 62])\n",
    "temp_test_target = np.array([31, 79, 78, 72, 64, 19,  9, 55,  1, 30, 31, 83, 57, 73,  9,  6, 14, 79,\n",
    "        10, 55, 73,  9, 41, 55, 78, 73, 66, 68, 72, 80, 55,  1, 14, 79, 75, 14,\n",
    "        55, 19, 55,  1, 30,  9, 41, 72, 31, 82, 41, 64, 27, 30, 68, 66, 41, 55,\n",
    "        77, 72, 55,  6, 30, 55, 31, 73, 31, 72, 66, 93, 55, 73,  9, 41, 55, 72,\n",
    "        50, 73, 75, 62, 66, 93, 55,  1, 31, 30, 68, 51, 14, 62, 48, 64, 45, 79,\n",
    "         9, 75, 72, 55, 62, 14, 72, 55, 62, 31, 68, 72, 55, 66, 79, 43, 72, 55,\n",
    "        30,  9, 82, 62, 55,  1, 73,  6, 92, 64, 64, 39, 42, 45, 74, 52, 53, 90,\n",
    "        53, 45])\n",
    "temp_test_input_char = []\n",
    "temp_test_target_char = []\n",
    "for i in range(len(temp_test_input)):\n",
    "    temp_test_input_char.append(index_to_char[temp_test_input[i]])\n",
    "for i in range(len(temp_test_target)):\n",
    "    temp_test_target_char.append(index_to_char[temp_test_target[i]])\n",
    "print(\">Input:\")\n",
    "print(''.join(temp_test_input_char))\n",
    "print(\">Target:\")\n",
    "print(''.join(temp_test_target_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save the accuracies and errors and .npy file\n",
    "np.save(f'{dir_root}/attention_training_losses.npy', training_losses)\n",
    "np.save(f'{dir_root}/attention_validation_losses.npy', validation_losses)\n",
    "np.save(f'{dir_root}/attention_training_accuracies.npy', training_accuracies)\n",
    "np.save(f'{dir_root}/attention_validation_accuracies.npy', validation_accuracies)\n",
    "np.save(f'{dir_root}/attention_update_counts.npy', update_counts)\n",
    "\n",
    "# Calculate errors: error = 1 - accuracy\n",
    "training_errors = [1 - acc for acc in training_accuracies]\n",
    "validation_errors = [1 - acc for acc in validation_accuracies]\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(update_counts, training_losses, label='Training Loss')\n",
    "plt.plot(range(0, len(validation_losses) * 1000, 1000), validation_losses, label='Validation Loss') \n",
    "plt.xlabel('Weight Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss vs. Weight Updates')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation error\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(update_counts, training_errors, label='Training Error')\n",
    "plt.plot(range(0, len(validation_errors) * 1000, 1000), validation_errors, label='Validation Error')  \n",
    "plt.xlabel('Weight Updates')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training and Validation Error vs. Weight Updates')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c109614febf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = f'{dir_root}/char_attention_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e976a5ab339735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = TransformerModel(vocab_size).to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_transformer(model, start_text, char_to_index, index_to_char, max_length=1024, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert start text to indices\n",
    "        context = torch.tensor([[char_to_index[c] for c in start_text]], dtype=torch.long).to(device)\n",
    "        generated_text = start_text\n",
    "\n",
    "        # Generate one character at a time\n",
    "        for i in range(max_length - len(start_text)):\n",
    "            # Create attention mask\n",
    "            seq_len = context.size(1)\n",
    "            src_mask = generate_square_subsequent_mask(seq_len).to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            output = model(context, src_mask)\n",
    "            \n",
    "            # Get predictions for the next character\n",
    "            next_token_logits = output[0, -1, :] / temperature\n",
    "            # Apply softmax to convert logits to probabilities\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(next_token_probs, 1)[0]\n",
    "            \n",
    "            # Convert to character and append to result\n",
    "            if next_token.item() in index_to_char:\n",
    "                next_char = index_to_char[next_token.item()]\n",
    "                generated_text += next_char\n",
    "                \n",
    "                # Append the new token to the context\n",
    "                context = torch.cat([context, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # Optional: Truncate context if it gets too long\n",
    "                if context.size(1) >= 128:  # Keep last 128 characters as context\n",
    "                    context = context[:, -128:]\n",
    "            \n",
    "    return generated_text\n",
    "\n",
    "# Save the trained model\n",
    "model_path = f'{dir_root}/transformer_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'char_to_index': char_to_index,\n",
    "    'index_to_char': index_to_char\n",
    "}, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load the model for generation\n",
    "def load_model_for_generation(model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model = TransformerModel(checkpoint['vocab_size']).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    return model, checkpoint['char_to_index'], checkpoint['index_to_char']\n",
    "\n",
    "# Example usage:\n",
    "# Load model and generate text\n",
    "loaded_model, loaded_char_to_index, loaded_index_to_char = load_model_for_generation(model_path)\n",
    "\n",
    "# Generate text with different temperatures\n",
    "test_prompts = [\n",
    "    \"I never shall forget that night.\",\n",
    "    \"The quick brown fox\",\n",
    "    \"Once upon a time\",\n",
    "]\n",
    "\n",
    "print(\"\\nGenerating text with different temperatures:\")\n",
    "temperatures = [0.3, 0.7, 1.0]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    for temp in temperatures:\n",
    "        generated = generate_text_transformer(\n",
    "            loaded_model, \n",
    "            prompt, \n",
    "            loaded_char_to_index, \n",
    "            loaded_index_to_char, \n",
    "            max_length=1024, \n",
    "            temperature=temp\n",
    "        )\n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        print(generated)\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2abf0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down if it's google colab\n",
    "# First sleep for a while so that changes to the notebook are saved\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
