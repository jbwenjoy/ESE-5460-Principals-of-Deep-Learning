{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1462a7a5d638814e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:02:42.747313Z",
     "start_time": "2024-11-02T23:02:42.731684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "dir_root = '.'\n",
    "# If using google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    dir_root = '/content/drive/MyDrive/Colab Notebooks/ESE546/hw3'\n",
    "\n",
    "print(dir_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302b430103d02bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.047920Z",
     "start_time": "2024-11-02T22:25:23.798288Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "302b430103d02bbe",
    "outputId": "baaf358c-4e61-453a-b1f6-262d1a218350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text from file: pg100.txt\n",
      "Loading text from file: pg2600.txt\n",
      "Loading text from file: pg766.txt\n",
      "Vocabulary size for each text: [107, 112, 90]\n",
      "Unique characters for each text: [{'L', 'T', ']', 'é', '6', 'e', 'l', 'N', 'v', 'Z', 'H', '-', 'ê', '“', 'o', '1', '8', '”', '[', 'b', 'O', '*', 'J', '7', '/', 'É', '9', '’', 'æ', ' ', 'P', 'W', '_', 'À', '$', 'E', 'r', 't', '?', 'ç', '•', 'A', '—', 'â', '&', 'G', 'R', 'S', '4', 'k', 'j', 'u', 'ë', ',', '0', 'Ç', '#', 'p', '.', ')', 'è', 'I', 'g', 'y', 'œ', 'B', 'à', 'h', '\\ufeff', ';', 'Q', '3', 'î', '%', 'Æ', '(', 'w', '\\n', 'Y', '‘', 'a', 'n', '2', 'x', 'm', 's', '5', 'i', 'M', 'F', 'K', ':', 'd', 'c', 'f', \"'\", 'X', 'q', 'D', '!', '™', 'C', '\\t', '…', 'z', 'V', 'U'}, {'L', 'T', ']', 'é', '6', 'e', 'l', 'N', '=', 'v', 'ó', 'Z', 'H', '-', 'ê', '“', 'o', '1', '8', '”', '[', 'b', 'O', '*', 'ú', 'J', '7', '/', 'ö', 'É', '9', '’', 'æ', ' ', 'P', 'W', 'À', '$', 'E', 'r', 't', '?', 'ç', 'ï', '•', 'A', '—', 'ý', 'â', 'Á', 'G', 'R', 'S', 'ü', '4', 'k', 'j', 'u', 'ë', ',', '0', '#', 'p', '.', ')', 'è', 'I', 'g', 'y', 'œ', 'B', 'à', 'h', '\\ufeff', ';', 'Q', '3', 'î', '%', '(', 'w', '\\n', 'Y', 'í', '‘', 'a', 'n', '2', 'x', 'm', 's', '5', 'ä', 'i', 'M', 'F', 'K', ':', 'd', 'ô', 'c', 'f', 'X', 'q', 'D', '!', '™', 'á', 'C', 'z', 'V', 'U'}, {'L', 'T', ']', '6', 'e', 'l', 'N', 'v', 'Z', 'H', '-', '“', 'o', '1', '8', '”', '[', 'b', 'O', '*', 'J', '7', '/', '9', '’', ' ', 'P', 'W', '_', '$', 'E', 'r', 't', '?', '•', 'A', '—', '&', 'G', 'R', 'S', '4', 'k', 'j', 'u', ',', '0', '#', 'p', '.', ')', 'I', 'g', 'y', 'B', 'h', '\\ufeff', ';', 'Q', '3', '%', '(', 'w', '\\n', 'Y', '‘', 'a', 'n', '2', 'x', 'm', 's', '5', 'i', 'M', 'F', 'K', ':', 'd', 'c', 'f', 'X', 'q', 'D', '!', '™', 'C', 'z', 'V', 'U'}]\n",
      "Example unique characters: {'L', 'T', ']', 'é', '6', 'e', 'l', 'N', 'v', 'Z', 'H', '-', 'ê', '“', 'o', '1', '8', '”', '[', 'b', 'O', '*', 'J', '7', '/', 'É', '9', '’', 'æ', ' ', 'P', 'W', '_', 'À', '$', 'E', 'r', 't', '?', 'ç', '•', 'A', '—', 'â', '&', 'G', 'R', 'S', '4', 'k', 'j', 'u', 'ë', ',', '0', 'Ç', '#', 'p', '.', ')', 'è', 'I', 'g', 'y', 'œ', 'B', 'à', 'h', '\\ufeff', ';', 'Q', '3', 'î', '%', 'Æ', '(', 'w', '\\n', 'Y', '‘', 'a', 'n', '2', 'x', 'm', 's', '5', 'i', 'M', 'F', 'K', ':', 'd', 'c', 'f', \"'\", 'X', 'q', 'D', '!', '™', 'C', '\\t', '…', 'z', 'V', 'U'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Load the text from a local file\n",
    "def load_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# Load the text from a URL\n",
    "def load_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    text = response.text.replace('\\r\\n', '\\n')  # Normalize line endings\n",
    "    return text\n",
    "\n",
    "# Count unique characters in the text\n",
    "def count_unique_chars(text):\n",
    "    unique_chars = set(text)\n",
    "    # Num of unique characters\n",
    "    vocab_size = len(unique_chars)\n",
    "    return vocab_size, unique_chars\n",
    "\n",
    "# List of file paths or URLs\n",
    "local_file_1 = 'pg100.txt'\n",
    "local_file_2 = 'pg2600.txt'\n",
    "local_file_3 = 'pg766.txt'\n",
    "url_file_1 = 'https://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "url_file_2 = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "url_file_3 = 'https://www.gutenberg.org/cache/epub/766/pg766.txt'\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    file_path_list = [url_file_1, url_file_2, url_file_3]\n",
    "else:\n",
    "    file_path_list = [local_file_1, local_file_2, local_file_3]\n",
    "text_list = []\n",
    "vocab_size_list = []\n",
    "unique_chars_list = []\n",
    "\n",
    "for file_path in file_path_list:\n",
    "    if file_path.startswith('http'):\n",
    "        print(f'Loading text from URL: {file_path}')\n",
    "        text = load_text_from_url(file_path)\n",
    "    else:\n",
    "        print(f'Loading text from file: {file_path}')\n",
    "        text = load_text_from_file(file_path)\n",
    "    vocab_size, unique_chars = count_unique_chars(text)\n",
    "    text_list.append(text)\n",
    "    vocab_size_list.append(vocab_size)\n",
    "    unique_chars_list.append(unique_chars)\n",
    "\n",
    "print(f'Vocabulary size for each text: {vocab_size_list}')\n",
    "print(f'Unique characters for each text: {unique_chars_list}')\n",
    "print(f'Example unique characters: {unique_chars_list[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ba040297cb3e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.063555Z",
     "start_time": "2024-11-02T22:25:24.047920Z"
    },
    "id": "b1ba040297cb3e10"
   },
   "outputs": [],
   "source": [
    "# # Compare unique characters from local and URL text files\n",
    "# local_file_path = 'pg2600.txt'\n",
    "# url_file_path = 'https://www.gutenberg.org/cache/epub/2600/pg2600.txt'\n",
    "#\n",
    "# local_text = load_text_from_file(local_file_path)\n",
    "# local_vocab_size, local_unique_chars = count_unique_chars(local_text)\n",
    "# url_text = load_text_from_url(url_file_path)\n",
    "# url_vocab_size, url_unique_chars = count_unique_chars(url_text)\n",
    "#\n",
    "# # Find the extra character(s)\n",
    "# extra_chars_in_url = url_unique_chars - local_unique_chars\n",
    "# extra_chars_in_local = local_unique_chars - url_unique_chars\n",
    "# print(f'Extra characters in URL text: {extra_chars_in_url}')\n",
    "# print(f'Extra characters in local text: {extra_chars_in_local}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e595766dd8deb1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.364527Z",
     "start_time": "2024-11-02T22:25:24.348901Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e595766dd8deb1b",
    "outputId": "e4bba092-0ed1-4fb8-f5bb-712821c6766a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to index mapping for first text: {'L': 0, 'T': 1, ']': 2, 'é': 3, '6': 4, 'e': 5, 'l': 6, 'N': 7, 'v': 8, 'Z': 9, 'H': 10, '-': 11, 'ê': 12, '“': 13, 'o': 14, '1': 15, '8': 16, '”': 17, '[': 18, 'b': 19, 'O': 20, '*': 21, 'J': 22, '7': 23, '/': 24, 'É': 25, '9': 26, '’': 27, 'æ': 28, ' ': 29, 'P': 30, 'W': 31, '_': 32, 'À': 33, '$': 34, 'E': 35, 'r': 36, 't': 37, '?': 38, 'ç': 39, '•': 40, 'A': 41, '—': 42, 'â': 43, '&': 44, 'G': 45, 'R': 46, 'S': 47, '4': 48, 'k': 49, 'j': 50, 'u': 51, 'ë': 52, ',': 53, '0': 54, 'Ç': 55, '#': 56, 'p': 57, '.': 58, ')': 59, 'è': 60, 'I': 61, 'g': 62, 'y': 63, 'œ': 64, 'B': 65, 'à': 66, 'h': 67, '\\ufeff': 68, ';': 69, 'Q': 70, '3': 71, 'î': 72, '%': 73, 'Æ': 74, '(': 75, 'w': 76, '\\n': 77, 'Y': 78, '‘': 79, 'a': 80, 'n': 81, '2': 82, 'x': 83, 'm': 84, 's': 85, '5': 86, 'i': 87, 'M': 88, 'F': 89, 'K': 90, ':': 91, 'd': 92, 'c': 93, 'f': 94, \"'\": 95, 'X': 96, 'q': 97, 'D': 98, '!': 99, '™': 100, 'C': 101, '\\t': 102, '…': 103, 'z': 104, 'V': 105, 'U': 106}\n",
      "Index to character mapping for first text: {0: 'L', 1: 'T', 2: ']', 3: 'é', 4: '6', 5: 'e', 6: 'l', 7: 'N', 8: 'v', 9: 'Z', 10: 'H', 11: '-', 12: 'ê', 13: '“', 14: 'o', 15: '1', 16: '8', 17: '”', 18: '[', 19: 'b', 20: 'O', 21: '*', 22: 'J', 23: '7', 24: '/', 25: 'É', 26: '9', 27: '’', 28: 'æ', 29: ' ', 30: 'P', 31: 'W', 32: '_', 33: 'À', 34: '$', 35: 'E', 36: 'r', 37: 't', 38: '?', 39: 'ç', 40: '•', 41: 'A', 42: '—', 43: 'â', 44: '&', 45: 'G', 46: 'R', 47: 'S', 48: '4', 49: 'k', 50: 'j', 51: 'u', 52: 'ë', 53: ',', 54: '0', 55: 'Ç', 56: '#', 57: 'p', 58: '.', 59: ')', 60: 'è', 61: 'I', 62: 'g', 63: 'y', 64: 'œ', 65: 'B', 66: 'à', 67: 'h', 68: '\\ufeff', 69: ';', 70: 'Q', 71: '3', 72: 'î', 73: '%', 74: 'Æ', 75: '(', 76: 'w', 77: '\\n', 78: 'Y', 79: '‘', 80: 'a', 81: 'n', 82: '2', 83: 'x', 84: 'm', 85: 's', 86: '5', 87: 'i', 88: 'M', 89: 'F', 90: 'K', 91: ':', 92: 'd', 93: 'c', 94: 'f', 95: \"'\", 96: 'X', 97: 'q', 98: 'D', 99: '!', 100: '™', 101: 'C', 102: '\\t', 103: '…', 104: 'z', 105: 'V', 106: 'U'}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to map characters to indices and vice-versa\n",
    "def create_char_mappings(unique_chars):\n",
    "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    index_to_char = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "vocab_size, unique_chars = vocab_size_list[0], unique_chars_list[0]\n",
    "char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "\n",
    "print(f\"Character to index mapping for first text: {char_to_index}\")\n",
    "print(f\"Index to character mapping for first text: {index_to_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff240b4b6efa2ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.489527Z",
     "start_time": "2024-11-02T22:25:24.380152Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cff240b4b6efa2ea",
    "outputId": "6b2b808a-d632-4e34-e632-cadd9f71be6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for 'a': [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# One-hot encode a character based on the character index\n",
    "def one_hot_encode(char, char_to_index, vocab_size):\n",
    "    one_hot_vector = np.zeros(vocab_size)\n",
    "    one_hot_vector[char_to_index[char]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "test_char_a = 'a'\n",
    "one_hot_vector = one_hot_encode(test_char_a, char_to_index, vocab_size)\n",
    "print(f\"One-hot encoding for '{test_char_a}': {one_hot_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ac26dff8806f1e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.520779Z",
     "start_time": "2024-11-02T22:25:24.505153Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ac26dff8806f1e0",
    "outputId": "2b139668-1682-45d1-aaf9-029a8f729b6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ill:\n",
      "Some in their hawks and hounds, some in their horse.\n",
      "And every humour hath his adjunct pleasure,\n",
      "Wherein it finds a joy above the rest,\n",
      "But these particulars are not my measure,\n",
      "All these I better in one general best.\n",
      "Thy love is better than high birth to me,\n",
      "Richer than wealth, prouder than garments’ costs,\n",
      "Of more delight than hawks and horses be:\n",
      "And having thee, of all men’s pride I boast.\n",
      "  Wretched in this alone, that thou mayst take,\n",
      "  All this away, and me most wretched make.\n",
      "\n",
      "\n",
      "                    92\n",
      "\n",
      "But do thy worst to steal thyself away,\n",
      "For term of life thou art assured mine,\n",
      "And life no longer than thy love will stay,\n",
      "For it depends upon that love of thine.\n",
      "Then need I not to fear the worst of wrongs,\n",
      "When in the least of them my life hath end,\n",
      "I see, a better state to me belongs\n",
      "Than that, which on thy humour doth depend.\n",
      "Thou canst not vex me with inconstant mind,\n",
      "Since that my life on thy revolt doth lie,\n",
      "O what a happy title do I find,\n",
      "Happy to have thy love, hap\n"
     ]
    }
   ],
   "source": [
    "# Sample a short portion from the first book\n",
    "text = text_list[0][60000:61000]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da41e402067fbaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.567651Z",
     "start_time": "2024-11-02T22:25:24.536404Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Generate sequences of 32 characters and the next character as the target\n",
    "def generate_sequences(text, char_to_index, sequence_length=32, stride=1):\n",
    "    input_sequences = []\n",
    "    target_characters = []\n",
    "\n",
    "    # for i in range(0, len(text) - sequence_length, stride):\n",
    "    for i in tqdm(range(0, len(text) - sequence_length - 1, stride), desc=\"Generating sequences\"):\n",
    "        input_seq = text[i:i+sequence_length]\n",
    "        # Target/next character\n",
    "        target_char = text[i+sequence_length]\n",
    "\n",
    "        # Convert input sequence to one-hot encoded vectors\n",
    "        input_seq_encoded = [one_hot_encode(c, char_to_index, len(char_to_index)) for c in input_seq]\n",
    "        input_sequences.append(input_seq_encoded)\n",
    "\n",
    "        # One-hot encoding for the target character\n",
    "        target_char_encoded = one_hot_encode(target_char, char_to_index, len(char_to_index))\n",
    "        target_characters.append(target_char_encoded)\n",
    "\n",
    "    return np.array(input_sequences), np.array(target_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b61dfa3f256a14a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:24.664626Z",
     "start_time": "2024-11-02T22:25:24.583278Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b61dfa3f256a14a8",
    "outputId": "b88f80aa-823f-48d0-b5d4-3322bb313126"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences:   0%|          | 0/967 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 967/967 [00:00<00:00, 29771.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (967, 32, 107)\n",
      "Target characters shape: (967, 107)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_sequences, target_characters = generate_sequences(text, char_to_index)\n",
    "print(f\"Input sequences shape: {input_sequences.shape}\")\n",
    "print(f\"Target characters shape: {target_characters.shape}\")\n",
    "print(input_sequences[2])\n",
    "print(target_characters[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b61aff6c6ca7909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:36:32.178217Z",
     "start_time": "2024-11-02T22:36:32.162592Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b61aff6c6ca7909",
    "outputId": "c9a4ddaa-39c9-4fb7-cf16-2b90e015c042"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(vocab_size, self.hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return F.log_softmax(out, dim=2), hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "def repackage_hidden(hidden_layer):\n",
    "    # Detach hidden states from their history\n",
    "    if isinstance(hidden_layer, torch.Tensor):\n",
    "        return hidden_layer.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2895b85ad94231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T22:25:33.308815Z",
     "start_time": "2024-11-02T22:25:27.152139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 84042/84042 [00:02<00:00, 31995.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: torch.Size([84042, 32, 107])\n",
      "Target characters shape: torch.Size([84042, 107])\n"
     ]
    }
   ],
   "source": [
    "USE_ALL_BOOKS = False\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    USE_ALL_BOOKS = True\n",
    "\n",
    "if USE_ALL_BOOKS:\n",
    "    # Load text and prepare data\n",
    "    all_texts = ''.join(text_list)  # Concatenate all texts\n",
    "    vocab_size, unique_chars = count_unique_chars(all_texts)\n",
    "    char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "    \n",
    "    # Generate sequences\n",
    "    input_sequences, target_characters = generate_sequences(all_texts, char_to_index, stride=16)\n",
    "    input_sequences = torch.tensor(input_sequences, dtype=torch.float32)\n",
    "    target_characters = torch.tensor(target_characters, dtype=torch.long)\n",
    "    \n",
    "    # Create TensorDataset and DataLoader\n",
    "    dataset = TensorDataset(input_sequences, target_characters)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print(\"Input sequences shape:\", input_sequences.shape)\n",
    "    print(\"Target characters shape:\", target_characters.shape)\n",
    "else:\n",
    "    text = text_list[0]\n",
    "    vocab_size, unique_chars = vocab_size_list[0], unique_chars_list[0]\n",
    "    char_to_index, index_to_char = create_char_mappings(unique_chars)\n",
    "    \n",
    "    input_sequences, target_characters = generate_sequences(text, char_to_index, stride=64)\n",
    "    input_sequences = torch.tensor(input_sequences, dtype=torch.float32)\n",
    "    target_characters = torch.tensor(target_characters, dtype=torch.long)\n",
    "    \n",
    "    dataset = TensorDataset(input_sequences, target_characters)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print(\"Input sequences shape:\", input_sequences.shape)\n",
    "    print(\"Target characters shape:\", target_characters.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c3b38fd80144a4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:01:59.704122Z",
     "start_time": "2024-11-02T23:01:27.918946Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "4c3b38fd80144a4c",
    "outputId": "bc040813-dedc-4488-9e61-21e68017c642"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 4.662619755905544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 1/10 [00:03<00:33,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.661312505772812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 10/10 [00:31<00:00,  3.18s/it]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10000 if 'google.colab' in str(get_ipython()) else 10\n",
    "batch_size = 64\n",
    "dropout = 0.5\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CharRNN(vocab_size, hidden_size, dropout).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  # Adjust gamma as needed\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_seqs, target_seqs in train_loader:\n",
    "        batch_size = input_seqs.size(0)\n",
    "        input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "        hidden = model.init_hidden(batch_size).to(device)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(input_seqs, hidden)\n",
    "        # print(\"Output shape:\", output.shape)\n",
    "        # print(\"Target shape:\", target_seqs.shape)\n",
    "        loss = criterion(output, target_seqs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch >= 1000:\n",
    "        scheduler.step()\n",
    "    \n",
    "    if epoch % 100 == 0 or epoch >= num_epochs - 1:\n",
    "        print(f'Epoch {epoch+1}, Training Loss: {total_loss / len(train_loader)}')\n",
    "    if epoch % 1000 == 0 or epoch >= num_epochs - 1:\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for input_seqs, target_seqs in val_loader:\n",
    "                batch_size = input_seqs.size(0)\n",
    "                input_seqs, target_seqs = input_seqs.to(device), target_seqs.to(device)\n",
    "                hidden = model.init_hidden(batch_size).to(device)\n",
    "                hidden = repackage_hidden(hidden)\n",
    "                output, hidden = model(input_seqs, hidden)\n",
    "                val_loss += criterion(output, target_seqs).item()\n",
    "        \n",
    "            print(f'Validation Loss: {val_loss / len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31c109614febf416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:02:44.345075Z",
     "start_time": "2024-11-02T23:02:44.313827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./char_rnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_path = f'{dir_root}/char_rnn_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3e976a5ab339735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:01:59.860193Z",
     "start_time": "2024-11-02T23:01:59.839576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bw Jiang\\AppData\\Local\\Temp\\ipykernel_32960\\2194315477.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = CharRNN(vocab_size, hidden_size, dropout).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e5c69371b00f16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:30:17.683680Z",
     "start_time": "2024-11-02T23:30:17.668052Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_text, char_to_index, index_to_char, max_length=1000, temperature=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(1).to(device)\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        input_seq = start_text\n",
    "        generated_text = start_text\n",
    "\n",
    "        for i in range(max_length):\n",
    "            input_seq_encoded = torch.tensor([one_hot_encode(c, char_to_index, len(char_to_index)) for c in input_seq], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            output, hidden = model(input_seq_encoded, hidden)\n",
    "            output_dist = output.data.view(-1).div(temperature).exp()\n",
    "            top_char = torch.multinomial(output_dist, 1)[0]\n",
    "            \n",
    "            # Ensure the predicted index is within the valid range\n",
    "            if top_char.item() in index_to_char:\n",
    "                predicted_char = index_to_char[top_char.item()]\n",
    "            else:\n",
    "                predicted_char = ''  # Handle out-of-range index by skipping or using a placeholder\n",
    "\n",
    "            generated_text += predicted_char\n",
    "            input_seq = input_seq[1:] + predicted_char\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c720fb045661f97b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T23:30:35.790072Z",
     "start_time": "2024-11-02T23:30:35.660165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The    ‘X!d ”œrbb ilrb eeobrrS  M[-.GGXJ:J72CSæ’’’’’GC[[[S[[[XX.oMM’oUUUCo ”UUUXCCCEHyxneerrytiiRilyt\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the trained model\n",
    "start_text = 'The '\n",
    "generated_text = generate_text(model, start_text, char_to_index, index_to_char, max_length=100, temperature=0.1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119ac6153fba63d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
