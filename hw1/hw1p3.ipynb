{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:09.933656Z",
     "start_time": "2024-09-21T22:30:06.670867Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision as thv\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "def get_mnist_dataset(root='./mnist_data'):\n",
    "    # Check if the dataset already exists locally\n",
    "    if os.path.exists(f'{root}/train_data.pkl') and os.path.exists(f'{root}/val_data.pkl'):\n",
    "        print(\"Loading MNIST dataset from local storage...\")\n",
    "        with open(f'{root}/train_data.pkl', 'rb') as f:\n",
    "            train = pickle.load(f)\n",
    "        with open(f'{root}/val_data.pkl', 'rb') as f:\n",
    "            val = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Downloading MNIST dataset...\")\n",
    "        # Download the dataset\n",
    "        train = thv.datasets.MNIST(root, download=True, train=True)\n",
    "        val = thv.datasets.MNIST(root, download=True, train=False)\n",
    "        \n",
    "        # Save the dataset locally\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        with open(f'{root}/train_data.pkl', 'wb') as f:\n",
    "            pickle.dump(train, f)\n",
    "        with open(f'{root}/val_data.pkl', 'wb') as f:\n",
    "            pickle.dump(val, f)\n",
    "\n",
    "    print(f\"Training dataset shape: {train.data.shape}, Number of targets: {len(train.targets)}\")\n",
    "    print(f\"Validation dataset shape: {val.data.shape}, Number of targets: {len(val.targets)}\")\n",
    "\n",
    "    return train, val\n",
    "    \n",
    "def split_dataset(train, val):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    # Define the number of classes and target sizes\n",
    "    num_classes = 10\n",
    "    target_train_size = 30000\n",
    "    target_val_size = 5000\n",
    "\n",
    "    # Initialize lists to hold the indices for the refined datasets\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "\n",
    "    # Iterate over each class and collect indices for the split\n",
    "    for i in range(num_classes):\n",
    "        # Get indices of all examples from class i in the training and validation sets\n",
    "        train_class_indices = (train.targets == i).nonzero(as_tuple=True)[0]\n",
    "        val_class_indices = (val.targets == i).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Convert indices to numpy arrays for shuffling\n",
    "        train_class_indices_np = train_class_indices.numpy()\n",
    "        val_class_indices_np = val_class_indices.numpy()\n",
    "\n",
    "        # Shuffle indices\n",
    "        np.random.shuffle(train_class_indices_np)\n",
    "        np.random.shuffle(val_class_indices_np)\n",
    "\n",
    "        # Calculate split indices, initially aiming to round up\n",
    "        split_idx_train = (len(train_class_indices_np) + 1) // 2\n",
    "        split_idx_val = (len(val_class_indices_np) + 1) // 2\n",
    "\n",
    "        # Adjust split to avoid exceeding overall target sizes\n",
    "        if len(train_indices) + split_idx_train > target_train_size:\n",
    "            split_idx_train = target_train_size - len(train_indices)\n",
    "        if len(val_indices) + split_idx_val > target_val_size:\n",
    "            split_idx_val = target_val_size - len(val_indices)\n",
    "\n",
    "        # Add indices to lists\n",
    "        train_indices.extend(train_class_indices_np[:split_idx_train])\n",
    "        val_indices.extend(val_class_indices_np[:split_idx_val])\n",
    "\n",
    "        # Check if we have already reached the target sizes\n",
    "        if len(train_indices) == target_train_size and len(val_indices) == target_val_size:\n",
    "            break\n",
    "\n",
    "    # Create refined datasets\n",
    "    refined_train = Subset(train, train_indices)\n",
    "    refined_val = Subset(val, val_indices)\n",
    "\n",
    "    print(f\"Refined training dataset size: {len(refined_train)}\")\n",
    "    print(f\"Refined validation dataset size: {len(refined_val)}\")\n",
    "\n",
    "    return refined_train, refined_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:09.994916Z",
     "start_time": "2024-09-21T22:30:09.933656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset from local storage...\n",
      "Training dataset shape: torch.Size([60000, 28, 28]), Number of targets: 60000\n",
      "Validation dataset shape: torch.Size([10000, 28, 28]), Number of targets: 10000\n",
      "Refined training dataset size: 30000\n",
      "Refined validation dataset size: 5000\n",
      "Train Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./prob3_mnist_data\n",
      "    Split: Train\n",
      "Val Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./prob3_mnist_data\n",
      "    Split: Test\n",
      "Refined Train <torch.utils.data.dataset.Subset object at 0x000002040BBE1070>\n",
      "Refined Val <torch.utils.data.dataset.Subset object at 0x000002040BBE1040>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./prob3_mnist_data\"\n",
    "\n",
    "train, val = get_mnist_dataset(data_path)\n",
    "refined_train, refined_val = split_dataset(train, val)\n",
    "\n",
    "print(\"Train\", train)\n",
    "print(\"Val\", val)\n",
    "print(\"Refined Train\", refined_train)\n",
    "print(\"Refined Val\", refined_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.781594Z",
     "start_time": "2024-09-21T22:30:10.241054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_images(dataset, num_images=6):\n",
    "    # Making sure the dataset can be indexed directly\n",
    "    assert hasattr(dataset, '__getitem__'), \"Dataset must support indexing\"\n",
    "\n",
    "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 3))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, idx in zip(axes, indices):\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        # MNIST images are 1-channel images, need to be reshaped if in tensor form\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.numpy().squeeze()  # Convert to numpy and remove extra dimensions\n",
    "        \n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_images(refined_train, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "### Forward function\n",
    "For $\\beta$ samples, $\\alpha$ input features, and $c$ output features, the forward function is \n",
    "$$h^{(l+1)} = h^{(l)} W^{\\top} + b$$\n",
    "where: $h^l \\in R^{\\beta \\times \\alpha}$, $W \\in R^{c \\times \\alpha}$, $b \\in R^c$, $h^{l+1} \\in R^{\\beta \\times c}$\n",
    "\n",
    "### Backward function:\n",
    "$$\\frac{\\partial L}{\\partial h^l} = \\frac{\\partial L}{\\partial h^{l+1}} W$$\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial h^{l+1}}^{\\top} h^l$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^\\beta \\frac{\\partial L}{\\partial h_i^{l+1}}$$\n",
    "where: $\\frac{\\partial L}{\\partial h^{l+1}} \\in R^{\\beta \\times c}$, $\\frac{\\partial L}{\\partial W} \\in R^{c \\times \\alpha}$, $\\frac{\\partial L}{\\partial b} \\in R^c$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.794927Z",
     "start_time": "2024-09-21T22:30:10.787304Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import linear_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer\n",
    "\n",
    "### Forward function\n",
    "$$h^{(l+1)} = \\max(0, h^{(l)})$$\n",
    "where: $h^l \\in R^{\\beta \\times \\alpha}$, $h^{(l+1)} \\in R^{\\beta \\times \\alpha}$\n",
    "\n",
    "### Backward function\n",
    "$$\\frac{\\partial L}{\\partial h^l} = \\frac{\\partial L}{\\partial h^{l+1}} \\odot \\mathbb{1}(h^l > 0)$$\n",
    "where: $\\odot$ is element-wise multiplication, $\\mathbb{1}$ is the indicator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.807278Z",
     "start_time": "2024-09-21T22:30:10.802968Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import relu_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Softmax and Cross-entropy Loss Layer\n",
    "\n",
    "### Forward function\n",
    "$$h_k^{(l+1)} = \\frac{e^{h_k^{(l)}}}{\\sum_{k'} e^{h_{k'}^{(l)}}}$$\n",
    "$$l(y) = -\\log(h_y^{(l+1)})$$\n",
    "For a mini-batch of size $\\beta$, the loss is\n",
    "$$l({y_i}_{i=1,...,\\beta}) = -\\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\log(h_y^{(l+1)})$$\n",
    "\n",
    "Error is\n",
    "$$\\text{error} = \\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\mathbb{1}(y \\neq \\text{argmax}_k(h_k^{(l+1)}))$$\n",
    "\n",
    "### Backward function\n",
    "$$\\frac{\\partial L}{\\partial h^{(l)}} = \\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\frac{\\partial l(y_i)}{\\partial h^{(l)}}$$\n",
    "where: $\\frac{\\partial l(y_i)}{\\partial h^{(l)}} = h^{(l+1)} - \\mathbb{1}(y_i)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.817565Z",
     "start_time": "2024-09-21T22:30:10.813554Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import softmax_cross_entropy_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the backward pass of all three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:33:59.686448Z",
     "start_time": "2024-09-21T22:33:59.673843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.30988871  24.99237965  37.53063319  52.16424849]\n",
      " [-10.69011129 -24.00762035 -38.46936681 -50.83575151]]\n",
      "[[118.1 131.2 144.3]\n",
      " [118.1 131.2 144.3]]\n",
      "[[  0.05   0.15   0.25  -0.05  -0.15  -0.25]\n",
      " [  0.5    1.5    2.5   -0.5   -1.5   -2.5 ]\n",
      " [  1.     3.     5.    -1.    -3.    -5.  ]\n",
      " [  5.    15.    25.    -5.   -15.   -25.  ]\n",
      " [  0.05   0.15   0.25  -0.05  -0.15  -0.25]\n",
      " [  0.5    1.5    2.5   -0.5   -1.5   -2.5 ]\n",
      " [  1.     3.     5.    -1.    -3.    -5.  ]\n",
      " [  5.    15.    25.    -5.   -15.   -25.  ]]\n",
      "[ 0.2  2.   4.  20. ]\n"
     ]
    }
   ],
   "source": [
    "from layers import linear_t\n",
    "\n",
    "lin = linear_t(alpha=3, c=4)\n",
    "h_l = np.array([[0.5, 1.5, 2.5], [-0.5, -1.5, -2.5]])  # alpha = 3, beta = 2\n",
    "dh_lp1 = np.array([[0.1, 1, 2, 10], [0.1, 1, 2, 10]])  # c = 4, beta = 2\n",
    "lin.W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])  # c = 4, alpha = 3\n",
    "\n",
    "h_lp1 = lin.forward(h_l)\n",
    "lin.zero_grad()\n",
    "dh_l = lin.backward(dh_lp1)\n",
    "\n",
    "print(h_lp1)\n",
    "print(dh_l)\n",
    "print(lin.dW)\n",
    "print(lin.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:31:12.951204Z",
     "start_time": "2024-09-21T22:31:12.934899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1.5 0. ]\n",
      " [0.5 0.  2.5]]\n",
      "[[ 0.3  2.   0. ]\n",
      " [-1.  -0.  -1. ]]\n"
     ]
    }
   ],
   "source": [
    "from layers import relu_t\n",
    "\n",
    "relu = relu_t()\n",
    "h_l = np.array([[0.5, 1.5, -2.5], [0.5, -1.5, 2.5]])  # alpha = 3, beta = 2\n",
    "dh_lp1 = np.array([[0.3, 2, 1], [-1, -1, -1]])  \n",
    "\n",
    "h_lp1 = relu.forward(h_l)\n",
    "dh_l = relu.backward(dh_lp1)\n",
    "\n",
    "print(h_lp1)\n",
    "print(dh_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-21T22:35:31.208064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single sample\n",
      "[[0.09003057 0.24472847 0.66524096]]\n",
      "0.4076059644443803\n",
      "0.0\n",
      "[[ 0.09003057  0.24472847 -0.33475904]]\n",
      "\n",
      "Batched\n",
      "[[0.09003057 0.24472847 0.66524096]\n",
      " [0.26538793 0.01321289 0.72139918]]\n",
      "2.3670843028559254\n",
      "0.5\n",
      "[[ 0.04501529  0.12236424 -0.16737952]\n",
      " [ 0.13269396 -0.49339356  0.36069959]]\n"
     ]
    }
   ],
   "source": [
    "from layers import softmax_cross_entropy_t\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Single sample\n",
    "sce = softmax_cross_entropy_t()\n",
    "h_l = np.array([1, 2, 3])\n",
    "y = np.array([2])\n",
    "dh_lp1 = np.array([0.1, 1, 2])\n",
    "\n",
    "h_lp1, ell, error = sce.forward(h_l, y)\n",
    "dh_l = sce.backward(dh_lp1, y)\n",
    "\n",
    "print(\"\\nSingle sample\")\n",
    "print(h_lp1)\n",
    "print(ell)\n",
    "print(error)\n",
    "print(dh_l)\n",
    "\n",
    "# Batched samples\n",
    "sce = softmax_cross_entropy_t()\n",
    "h_l = np.array([[1, 2, 3], [2, -1, 3]])\n",
    "y = np.array([2, 1])\n",
    "dh_lp1 = np.array([[0.1, 1, 2], [0.1, 1, 2]])\n",
    "\n",
    "h_lp1, ell, error = sce.forward(h_l, y)\n",
    "dh_l = sce.backward(dh_lp1, y)\n",
    "\n",
    "print(\"\\nBatched\")\n",
    "print(h_lp1)\n",
    "print(ell)\n",
    "print(error)\n",
    "print(dh_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since all three layers have been validated and compared with PyTorch, we can continue to (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "...\n",
    "\n",
    "# Initialize all the layers\n",
    "l1, l2, l3 = linear_t(), relu_t(), softmax_cross_entropy_t()\n",
    "net = [l1, l2, l3]\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "# Train for at least 1000 iterations\n",
    "import tqdm\n",
    "for t in tqdm.tqdm(range(1000)):\n",
    "    # 1. Sample a mini-batch of size = 32\n",
    "    # Each image in the mini-batch is chosen uniformly randomly from the training dataset\n",
    "    x, y = ...\n",
    "    \n",
    "    # 2. Zero gradient buffer\n",
    "    for l in net:\n",
    "        l.zero_grad()\n",
    "    \n",
    "    # 3. Forward pass\n",
    "    h1 = l1.forward(x)\n",
    "    h2 = l2.forward(h1)\n",
    "    ell, error = l3.forward(h2, y)\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    dh2 = l3.backward()\n",
    "    dh1 = l2.backward(dh2)\n",
    "    dx = l1.backward(dh1)\n",
    "    \n",
    "    # 5. Gather backprop gradients\n",
    "    dW, db = l1.dW, l1.db\n",
    "    \n",
    "    # 6. Print some quantities for logging and debugging\n",
    "    print(t, ell, error)\n",
    "    print(t, np.linalg.norm(dW/l1.W), np.linalg.norm(db/l1.b))\n",
    "    \n",
    "    # 7. One step of SGD\n",
    "    l1.W = l1.W - lr * dW\n",
    "    l1.b = l1.b - lr * db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
