{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:09.933656Z",
     "start_time": "2024-09-21T22:30:06.670867Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision as thv\n",
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "def get_mnist_dataset(root='./mnist_data'):\n",
    "    # Check if the dataset already exists locally\n",
    "    if os.path.exists(f'{root}/train_data.pkl') and os.path.exists(f'{root}/val_data.pkl'):\n",
    "        print(\"Loading MNIST dataset from local storage...\")\n",
    "        with open(f'{root}/train_data.pkl', 'rb') as f:\n",
    "            train = pickle.load(f)\n",
    "        with open(f'{root}/val_data.pkl', 'rb') as f:\n",
    "            val = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Downloading MNIST dataset...\")\n",
    "        # Download the dataset\n",
    "        train = thv.datasets.MNIST(root, download=True, train=True)\n",
    "        val = thv.datasets.MNIST(root, download=True, train=False)\n",
    "        \n",
    "        # Save the dataset locally\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        with open(f'{root}/train_data.pkl', 'wb') as f:\n",
    "            pickle.dump(train, f)\n",
    "        with open(f'{root}/val_data.pkl', 'wb') as f:\n",
    "            pickle.dump(val, f)\n",
    "\n",
    "    print(f\"Training dataset shape: {train.data.shape}, Number of targets: {len(train.targets)}\")\n",
    "    print(f\"Validation dataset shape: {val.data.shape}, Number of targets: {len(val.targets)}\")\n",
    "\n",
    "    return train, val\n",
    "    \n",
    "def split_dataset(train, val):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    # Define the number of classes and target sizes\n",
    "    num_classes = 10\n",
    "    target_train_size = 30000\n",
    "    target_val_size = 5000\n",
    "\n",
    "    # Initialize lists to hold the indices for the refined datasets\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "\n",
    "    # Iterate over each class and collect indices for the split\n",
    "    for i in range(num_classes):\n",
    "        # Get indices of all examples from class i in the training and validation sets\n",
    "        train_class_indices = (train.targets == i).nonzero(as_tuple=True)[0]\n",
    "        val_class_indices = (val.targets == i).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Convert indices to numpy arrays for shuffling\n",
    "        train_class_indices_np = train_class_indices.numpy()\n",
    "        val_class_indices_np = val_class_indices.numpy()\n",
    "\n",
    "        # Shuffle indices\n",
    "        np.random.shuffle(train_class_indices_np)\n",
    "        np.random.shuffle(val_class_indices_np)\n",
    "\n",
    "        # Calculate split indices, initially aiming to round up\n",
    "        split_idx_train = (len(train_class_indices_np) + 1) // 2\n",
    "        split_idx_val = (len(val_class_indices_np) + 1) // 2\n",
    "\n",
    "        # Adjust split to avoid exceeding overall target sizes\n",
    "        if len(train_indices) + split_idx_train > target_train_size:\n",
    "            split_idx_train = target_train_size - len(train_indices)\n",
    "        if len(val_indices) + split_idx_val > target_val_size:\n",
    "            split_idx_val = target_val_size - len(val_indices)\n",
    "\n",
    "        # Add indices to lists\n",
    "        train_indices.extend(train_class_indices_np[:split_idx_train])\n",
    "        val_indices.extend(val_class_indices_np[:split_idx_val])\n",
    "\n",
    "        # Check if we have already reached the target sizes\n",
    "        if len(train_indices) == target_train_size and len(val_indices) == target_val_size:\n",
    "            break\n",
    "\n",
    "    # Create refined datasets\n",
    "    refined_train = Subset(train, train_indices)\n",
    "    refined_val = Subset(val, val_indices)\n",
    "\n",
    "    print(f\"Refined training dataset size: {len(refined_train)}\")\n",
    "    print(f\"Refined validation dataset size: {len(refined_val)}\")\n",
    "\n",
    "    return refined_train, refined_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:09.994916Z",
     "start_time": "2024-09-21T22:30:09.933656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset from local storage...\n",
      "Training dataset shape: torch.Size([60000, 28, 28]), Number of targets: 60000\n",
      "Validation dataset shape: torch.Size([10000, 28, 28]), Number of targets: 10000\n",
      "Refined training dataset size: 30000\n",
      "Refined validation dataset size: 5000\n",
      "Train Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./prob3_mnist_data\n",
      "    Split: Train\n",
      "Val Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./prob3_mnist_data\n",
      "    Split: Test\n",
      "Refined Train <torch.utils.data.dataset.Subset object at 0x00000218507B4FD0>\n",
      "Refined Val <torch.utils.data.dataset.Subset object at 0x00000218248F0220>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./prob3_mnist_data\"\n",
    "\n",
    "train, val = get_mnist_dataset(data_path)\n",
    "refined_train, refined_val = split_dataset(train, val)\n",
    "\n",
    "print(\"Train\", train)\n",
    "print(\"Val\", val)\n",
    "print(\"Refined Train\", refined_train)\n",
    "print(\"Refined Val\", refined_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.781594Z",
     "start_time": "2024-09-21T22:30:10.241054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_images(dataset, num_images=6):\n",
    "    # Making sure the dataset can be indexed directly\n",
    "    assert hasattr(dataset, '__getitem__'), \"Dataset must support indexing\"\n",
    "\n",
    "    indices = np.random.choice(len(dataset), num_images, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2, 3))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, idx in zip(axes, indices):\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        # MNIST images are 1-channel images, need to be reshaped if in tensor form\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.numpy().squeeze()  # Convert to numpy and remove extra dimensions\n",
    "        \n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_images(refined_train, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer\n",
    "\n",
    "### Forward function\n",
    "For $\\beta$ samples, $\\alpha$ input features, and $c$ output features, the forward function is \n",
    "$$h^{(l+1)} = h^{(l)} W^{\\top} + b$$\n",
    "where: $h^l \\in R^{\\beta \\times \\alpha}$, $W \\in R^{c \\times \\alpha}$, $b \\in R^c$, $h^{l+1} \\in R^{\\beta \\times c}$\n",
    "\n",
    "### Backward function:\n",
    "$$\\frac{\\partial L}{\\partial h^l} = \\frac{\\partial L}{\\partial h^{l+1}} W$$\n",
    "$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial h^{l+1}}^{\\top} h^l$$\n",
    "$$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^\\beta \\frac{\\partial L}{\\partial h_i^{l+1}}$$\n",
    "where: $\\frac{\\partial L}{\\partial h^{l+1}} \\in R^{\\beta \\times c}$, $\\frac{\\partial L}{\\partial W} \\in R^{c \\times \\alpha}$, $\\frac{\\partial L}{\\partial b} \\in R^c$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.794927Z",
     "start_time": "2024-09-21T22:30:10.787304Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import linear_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Layer\n",
    "\n",
    "### Forward function\n",
    "$$h^{(l+1)} = \\max(0, h^{(l)})$$\n",
    "where: $h^l \\in R^{\\beta \\times \\alpha}$, $h^{(l+1)} \\in R^{\\beta \\times \\alpha}$\n",
    "\n",
    "### Backward function\n",
    "$$\\frac{\\partial L}{\\partial h^l} = \\frac{\\partial L}{\\partial h^{l+1}} \\odot \\mathbb{1}(h^l > 0)$$\n",
    "where: $\\odot$ is element-wise multiplication, $\\mathbb{1}$ is the indicator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.807278Z",
     "start_time": "2024-09-21T22:30:10.802968Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import relu_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Softmax and Cross-entropy Loss Layer\n",
    "\n",
    "### Forward function\n",
    "$$h_k^{(l+1)} = \\frac{e^{h_k^{(l)}}}{\\sum_{k'} e^{h_{k'}^{(l)}}}$$\n",
    "$$l(y) = -\\log(h_y^{(l+1)})$$\n",
    "For a mini-batch of size $\\beta$, the loss is\n",
    "$$l({y_i}_{i=1,...,\\beta}) = -\\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\log(h_y^{(l+1)})$$\n",
    "\n",
    "Error is\n",
    "$$\\text{error} = \\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\mathbb{1}(y \\neq \\text{argmax}_k(h_k^{(l+1)}))$$\n",
    "\n",
    "### Backward function\n",
    "$$\\frac{\\partial L}{\\partial h^{(l)}} = \\frac{1}{\\beta} \\sum_{i=1}^{\\beta} \\frac{\\partial l(y_i)}{\\partial h^{(l)}}$$\n",
    "where: $\\frac{\\partial l(y_i)}{\\partial h^{(l)}} = h^{(l+1)} - \\mathbb{1}(y_i)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:30:10.817565Z",
     "start_time": "2024-09-21T22:30:10.813554Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import softmax_cross_entropy_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the backward pass of all three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:33:59.686448Z",
     "start_time": "2024-09-21T22:33:59.673843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.5070929   24.39600313  38.79318697  51.17922614]\n",
      " [-10.4929071  -24.60399687 -37.20681303 -51.82077386]]\n",
      "[[118.1 131.2 144.3]\n",
      " [118.1 131.2 144.3]]\n",
      "[[  0.05   0.15   0.25  -0.05  -0.15  -0.25]\n",
      " [  0.5    1.5    2.5   -0.5   -1.5   -2.5 ]\n",
      " [  1.     3.     5.    -1.    -3.    -5.  ]\n",
      " [  5.    15.    25.    -5.   -15.   -25.  ]\n",
      " [  0.05   0.15   0.25  -0.05  -0.15  -0.25]\n",
      " [  0.5    1.5    2.5   -0.5   -1.5   -2.5 ]\n",
      " [  1.     3.     5.    -1.    -3.    -5.  ]\n",
      " [  5.    15.    25.    -5.   -15.   -25.  ]]\n",
      "[ 0.2  2.   4.  20. ]\n"
     ]
    }
   ],
   "source": [
    "from layers import linear_t\n",
    "\n",
    "lin = linear_t(alpha=3, c=4)\n",
    "h_l = np.array([[0.5, 1.5, 2.5], [-0.5, -1.5, -2.5]])  # alpha = 3, beta = 2\n",
    "dh_lp1 = np.array([[0.1, 1, 2, 10], [0.1, 1, 2, 10]])  # c = 4, beta = 2\n",
    "lin.W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])  # c = 4, alpha = 3\n",
    "\n",
    "h_lp1 = lin.forward(h_l)\n",
    "lin.zero_grad()\n",
    "dh_l = lin.backward(dh_lp1)\n",
    "\n",
    "print(h_lp1)\n",
    "print(dh_l)\n",
    "print(lin.dW)\n",
    "print(lin.db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-21T22:31:12.951204Z",
     "start_time": "2024-09-21T22:31:12.934899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 1.5 0. ]\n",
      " [0.5 0.  2.5]]\n",
      "[[ 0.3  2.   0. ]\n",
      " [-1.  -0.  -1. ]]\n"
     ]
    }
   ],
   "source": [
    "from layers import relu_t\n",
    "\n",
    "relu = relu_t()\n",
    "h_l = np.array([[0.5, 1.5, -2.5], [0.5, -1.5, 2.5]])  # alpha = 3, beta = 2\n",
    "dh_lp1 = np.array([[0.3, 2, 1], [-1, -1, -1]])  \n",
    "\n",
    "h_lp1 = relu.forward(h_l)\n",
    "dh_l = relu.backward(dh_lp1)\n",
    "\n",
    "print(h_lp1)\n",
    "print(dh_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-21T22:35:31.208064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single sample\n",
      "[[0.09003057 0.24472847 0.66524096]]\n",
      "0.4076059644443803\n",
      "0.0\n",
      "[[ 0.09003057  0.24472847 -0.33475904]]\n",
      "\n",
      "Batched\n",
      "[[0.09003057 0.24472847 0.66524096]\n",
      " [0.26538793 0.01321289 0.72139918]]\n",
      "2.3670843028559254\n",
      "0.5\n",
      "[[ 0.04501529  0.12236424 -0.16737952]\n",
      " [ 0.13269396 -0.49339356  0.36069959]]\n"
     ]
    }
   ],
   "source": [
    "from layers import softmax_cross_entropy_t\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Single sample\n",
    "sce = softmax_cross_entropy_t()\n",
    "h_l = np.array([1, 2, 3])\n",
    "y = np.array([2])\n",
    "dh_lp1 = np.array([0.1, 1, 2])\n",
    "\n",
    "h_lp1, ell, error = sce.forward(h_l, y)\n",
    "dh_l = sce.backward(dh_lp1, y)\n",
    "\n",
    "print(\"\\nSingle sample\")\n",
    "print(h_lp1)\n",
    "print(ell)\n",
    "print(error)\n",
    "print(dh_l)\n",
    "\n",
    "# Batched samples\n",
    "sce = softmax_cross_entropy_t()\n",
    "h_l = np.array([[1, 2, 3], [2, -1, 3]])\n",
    "y = np.array([2, 1])\n",
    "dh_lp1 = np.array([[0.1, 1, 2], [0.1, 1, 2]])\n",
    "\n",
    "h_lp1, ell, error = sce.forward(h_l, y)\n",
    "dh_l = sce.backward(dh_lp1, y)\n",
    "\n",
    "print(\"\\nBatched\")\n",
    "print(h_lp1)\n",
    "print(ell)\n",
    "print(error)\n",
    "print(dh_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since all three layers have been validated and compared with PyTorch, we can continue to (f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36.919152750795895 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [02:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (320,25088) (10,784) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 6. Print some quantities for logging and debugging\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(t, ell, error)\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28mprint\u001b[39m(t, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mdW\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m), np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(db\u001b[38;5;241m/\u001b[39ml1\u001b[38;5;241m.\u001b[39mb))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 7. One step of SGD\u001b[39;00m\n\u001b[0;32m     51\u001b[0m l1\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m l1\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m dW\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (320,25088) (10,784) "
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "refined_train\n",
    "\n",
    "# Initialize all the layers\n",
    "l1, l2, l3 = linear_t(), relu_t(), softmax_cross_entropy_t()\n",
    "net = [l1, l2, l3]\n",
    "\n",
    "lr = 0.1\n",
    "np.random.seed(42)\n",
    "\n",
    "# Train for at least 1000 iterations\n",
    "import tqdm\n",
    "for t in tqdm.tqdm(range(1000)):\n",
    "    # 1. Sample a mini-batch of size = 32\n",
    "    # Each image in the mini-batch is chosen uniformly randomly from the training dataset\n",
    "    # Access an image and its label using: image, label = refined_train[idx]\n",
    "\n",
    "    # Extract 32 random indices\n",
    "    indices = np.random.choice(len(refined_train), 32, replace=False)\n",
    "\n",
    "    # Extract the images and labels\n",
    "    images_list = [np.array(refined_train[idx][0]) for idx in indices]  # PIL imgs to numpy arrays (28x28 each)\n",
    "    labels_list = [refined_train[idx][1] for idx in indices]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(images_list).reshape(32, -1)  # numpy array of shape (32, 784)\n",
    "    y = np.array(labels_list)\n",
    "    \n",
    "    # 2. Zero gradient buffer\n",
    "    for l in net:\n",
    "        l.zero_grad()\n",
    "    \n",
    "    # 3. Forward pass\n",
    "    h1 = l1.forward(x)\n",
    "    h2 = l2.forward(h1)\n",
    "    _, ell, error = l3.forward(h2, y)\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    dh2 = l3.backward()\n",
    "    dh1 = l2.backward(dh2)\n",
    "    dx = l1.backward(dh1)\n",
    "    \n",
    "    # 5. Gather backprop gradients\n",
    "    dW, db = l1.dW, l1.db\n",
    "    \n",
    "    # 6. Print some quantities for logging and debugging\n",
    "    print(t, ell, error)\n",
    "    print(t, np.linalg.norm(dW/l1.W), np.linalg.norm(db/l1.b))\n",
    "    \n",
    "    # 7. One step of SGD\n",
    "    l1.W = l1.W - lr * dW\n",
    "    l1.b = l1.b - lr * db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
