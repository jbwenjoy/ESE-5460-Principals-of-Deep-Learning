{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107cce774b4dc37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:44.136086Z",
     "start_time": "2024-11-27T18:30:34.170452Z"
    },
    "id": "9107cce774b4dc37"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys, subprocess, json, argparse\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a450805ac6a98a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:44.244065Z",
     "start_time": "2024-11-27T18:30:44.156611Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a450805ac6a98a5",
    "outputId": "7e2fa026-f82b-40c7-f5fd-8d9890481bd4"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "dir_root = ''\n",
    "# If using google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    dir_root = '/content/drive/MyDrive/Colab Notebooks/ESE546/hw4'\n",
    "\n",
    "print(\"dir_root:\", dir_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599442d8304e9e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:47.892471Z",
     "start_time": "2024-11-27T18:30:46.808491Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c599442d8304e9e8",
    "outputId": "8545f3ab-5fa5-4dc0-871f-8ae64b5ad9fa"
   },
   "outputs": [],
   "source": [
    "# Define the dataset directory\n",
    "data_dir = os.path.join(dir_root, 'data')\n",
    "print(data_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, 'cifar-10-batches-py')):\n",
    "    download = True\n",
    "    print('Dataset not found, downloading...')\n",
    "else:\n",
    "    download = False\n",
    "    print('Dataset found, not downloading.')\n",
    "\n",
    "# Reading in the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True,\n",
    "                                        download=download, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "                                       download=download, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a904e76d2d9a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:31:31.890284Z",
     "start_time": "2024-11-27T18:31:31.874654Z"
    },
    "id": "73a904e76d2d9a97"
   },
   "outputs": [],
   "source": [
    "# File allcnn.py provided by Prof. Pratik Chaudhari\n",
    "# at https://gist.github.com/pratikac/68d6d94e4739786798e90691fb1a581b\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, o):\n",
    "        super().__init__()\n",
    "        self.o = o\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, self.o)\n",
    "\n",
    "class allcnn_t(nn.Module):\n",
    "    def __init__(self, c1=96, c2=192):\n",
    "        super().__init__()\n",
    "        d = 0.5\n",
    "\n",
    "        def convbn(ci, co, ksz, s=1, pz=0):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, ksz, stride=s, padding=pz),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(co))\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            convbn(3, c1, 3, 1, 1),\n",
    "            convbn(c1, c1, 3, 1, 1),\n",
    "            convbn(c1, c1, 3, 2, 1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c1, c2, 3, 1, 1),\n",
    "            convbn(c2, c2, 3, 1, 1),\n",
    "            convbn(c2, c2, 3, 2, 1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c2, c2, 3, 1, 1),\n",
    "            convbn(c2, c2, 3, 1, 1),\n",
    "            convbn(c2, 10, 1, 1),\n",
    "            nn.AvgPool2d(8),\n",
    "            View(10))\n",
    "\n",
    "        print('Num parameters: ', sum([p.numel() for p in self.m.parameters()]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.m(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53373226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decay = 1e-3\n",
    "# SGD with Nesterov’s momentum of 0.9\n",
    "# Dropout = 0.5\n",
    "# Learning rate starts with eta_0 = 1e-5,\n",
    "# then eta_tp1 = 1.1 * eta_t or 1.05 * eta_t (t <= 100)\n",
    "\n",
    "# First train 100 iters\n",
    "# Record the average training loss of each mini-batch separately and the learning rate that was used for it for about 100 iterations.\n",
    "# Plot the training loss (Y-axis) as a function of the learning rate (X-axis); use a log-scale for the X-axis.\n",
    "\n",
    "def learning_rate_schedule(optimizer, rate=1.1):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= rate\n",
    "\n",
    "def train(\n",
    "    net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot, logger\n",
    "):\n",
    "    model = net.to(device)\n",
    "    total_step = len(train_loader)\n",
    "    overall_step = 0\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        flag = 0\n",
    "        running_loss = 0.0\n",
    "        learning_rates.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if (i + 1) % 130 == 0:\n",
    "                print(\n",
    "                    \"Epoch [{}/{}], Step [{}/{}], Learning Rate: {:.3g}, Loss: {:.4f}\".format(\n",
    "                        epoch + 1,\n",
    "                        epochs,\n",
    "                        i + 1,\n",
    "                        total_step,\n",
    "                        optimizer.param_groups[0][\"lr\"],\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "            if plot:\n",
    "                info = {(\"loss_\" + model_name): loss.item()}\n",
    "\n",
    "                for tag, value in info.items():\n",
    "                    logger.add_scalar(tag, value, overall_step + 1)\n",
    "\n",
    "        # Update learning rate every epoch\n",
    "        learning_rate_schedule(optimizer, rate=1.1)\n",
    "\n",
    "        train_loss_values.append(running_loss)\n",
    "        train_error.append(100 - 100 * correct / total)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Validation Loss: {:.4f},'.format(val_running_loss), 'Validation Error {} %'.format(100 - 100 * correct / total))\n",
    "        val_error.append(100 - 100 * correct / total)\n",
    "        val_loss_values.append(val_running_loss)\n",
    "\n",
    "    return val_error, val_loss_values, train_error, train_loss_values, learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71255eb6cdf29300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T18:32:10.874838Z",
     "start_time": "2024-11-27T18:32:10.843590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71255eb6cdf29300",
    "outputId": "60b6a7a4-0d29-43ec-c79e-e18abbb07b72"
   },
   "outputs": [],
   "source": [
    "# Weight decay = 1e-3\n",
    "# SGD with Nesterov’s momentum of 0.9\n",
    "# Dropout = 0.5\n",
    "# Learning rate starts with eta_0 = 1e-5,\n",
    "# then eta_tp1 = 1.1 * eta_t or 1.05 * eta_t (t <= 100)\n",
    "\n",
    "model_path_100 = os.path.join(dir_root, 'runs/hw4p3_model_100_epoch.pt')\n",
    "\n",
    "TRAIN_FLAG = True\n",
    "\n",
    "if not os.path.exists(model_path_100) or TRAIN_FLAG:\n",
    "    model = allcnn_t().to(device)\n",
    "    epochs = 100\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "    logger_1 = SummaryWriter(os.path.join(dir_root, \"runs/cnn_experiment\"))\n",
    "\n",
    "    val_error, val_loss_values, train_error, train_loss_values, learning_rates = train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        trainloader,\n",
    "        testloader,\n",
    "        epochs,\n",
    "        \"cnn_curve_100\",\n",
    "        True,\n",
    "        logger_1,\n",
    "    )\n",
    "\n",
    "    torch.save(model, model_path_100)\n",
    "\n",
    "    np.save(os.path.join(dir_root, 'runs/train_error_100.npy'), np.array(train_error))\n",
    "    np.save(os.path.join(dir_root, 'runs/train_loss_values_100.npy'), np.array(train_loss_values))\n",
    "    np.save(os.path.join(dir_root, 'runs/val_error_100.npy'), np.array(val_error))\n",
    "    np.save(os.path.join(dir_root, 'runs/val_loss_values_100.npy'), np.array(val_loss_values))\n",
    "    np.save(os.path.join(dir_root, 'runs/learning_rates_100.npy'), np.array(learning_rates))\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path_100}, skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce6693780a48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the npy files for plotting\n",
    "train_error_100 = np.load(os.path.join(dir_root, 'runs/train_error_100.npy'))\n",
    "train_loss_values_100 = np.load(os.path.join(dir_root, 'runs/train_loss_values_100.npy'))\n",
    "val_error_100 = np.load(os.path.join(dir_root, 'runs/val_error_100.npy'))\n",
    "val_loss_values_100 = np.load(os.path.join(dir_root, 'runs/val_loss_values_100.npy'))\n",
    "learning_rates_100 = np.load(os.path.join(dir_root, 'runs/learning_rates_100.npy'))\n",
    "\n",
    "# Plot the training loss (Y-axis) as a function of the learning rate (X-axis); use a log-scale for the X-axis.\n",
    "plt.figure()\n",
    "plt.plot(train_loss_values_100, learning_rates_100)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Training Loss')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Training Loss vs Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8439d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the minimum of training loss \n",
    "# The corresponding learning rate - eta_star\n",
    "eta_star = learning_rates_100[np.argmin(train_loss_values_100)]\n",
    "\n",
    "print(f\"Minimum training loss: {np.min(train_loss_values_100)}\")\n",
    "print(f\"Corresponding learning rate: {eta_star}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ed04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, use cosine learning rate schedule with a warmup\n",
    "# \\eta(t) = 1e-4 + t / T * eta_max, if t <= T0\n",
    "# \\eta(t) = eta_max * cos(pi / 2 * (t - T0) / (T - T0)) + 1e-6, if T0 < t <= T\n",
    "# with eta_max = eta_star / 10, 100 epochs, T0 = T / 5\n",
    "# Compute the validation loss and error after every epoch.\n",
    "\n",
    "eta_max = eta_star / 10\n",
    "\n",
    "def learning_rate_schedule(optimizer, t, T, eta_max=eta_max):\n",
    "    T0 = T / 5\n",
    "    if t <= T0:\n",
    "        lr_new = 1e-4 + t / T * eta_max\n",
    "    elif T0 < t <= T:\n",
    "        lr_new = eta_max * np.cos(np.pi / 2 * (t - T0) / (T - T0)) + 1e-6\n",
    "    else:  # do nothing and don't update the learning rate\n",
    "        return\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_new\n",
    "\n",
    "\n",
    "def train_eta_max(\n",
    "    net,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs,\n",
    "    model_name,\n",
    "    plot,\n",
    "    logger,\n",
    "):\n",
    "    model = net.to(device)\n",
    "    total_step = len(train_loader)\n",
    "    overall_step = 0\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        flag = 0\n",
    "        running_loss = 0.0\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if (i + 1) % 130 == 0:\n",
    "                print(\n",
    "                    \"Epoch [{}/{}], Step [{}/{}], Learning Rate: {:.3g}, Loss: {:.4f}\".format(\n",
    "                        epoch + 1,\n",
    "                        epochs,\n",
    "                        i + 1,\n",
    "                        total_step,\n",
    "                        optimizer.param_groups[0][\"lr\"],\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "            if plot:\n",
    "                info = { ('loss_' + model_name): loss.item() }\n",
    "\n",
    "                for tag, value in info.items():\n",
    "                    logger.add_scalar(tag, value, overall_step+1)\n",
    "\n",
    "        # Update learning rate every epoch\n",
    "        learning_rate_schedule(optimizer, epoch, epochs, eta_max)\n",
    "\n",
    "        train_loss_values.append(running_loss)\n",
    "        train_error.append(100 - 100 * correct / total)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Validation Loss: {:.4f},'.format(val_running_loss), 'Validation Error {} %'.format(100 - 100 * correct / total))\n",
    "        val_error.append(100 - 100 * correct / total)\n",
    "        val_loss_values.append(val_running_loss)\n",
    "        \n",
    "    return val_error, val_loss_values, train_error, train_loss_values, learning_rates\n",
    "\n",
    "\n",
    "model_path_100_eta_max = os.path.join(dir_root, 'runs/hw4p3_model_100_epoch_eta_max.pt')\n",
    "\n",
    "TRAIN_FLAG = True\n",
    "\n",
    "if not os.path.exists(model_path_100_eta_max) or TRAIN_FLAG:\n",
    "    model = allcnn_t().to(device)\n",
    "    epochs = 100\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=eta_max, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "    logger_eta_max = SummaryWriter(os.path.join(dir_root, 'runs/cnn_experiment_eta_max'))\n",
    "\n",
    "    val_error, val_loss_values, train_error, train_loss_values, learning_rates = (\n",
    "        train_eta_max(\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            trainloader,\n",
    "            testloader,\n",
    "            epochs,\n",
    "            \"cnn_curve_100_eta_max\",\n",
    "            True,\n",
    "            logger_eta_max,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    torch.save(model, model_path_100_eta_max)\n",
    "\n",
    "    np.save(os.path.join(dir_root, 'runs/train_error_100_eta_max.npy'), np.array(train_error))\n",
    "    np.save(os.path.join(dir_root, 'runs/train_loss_values_100_eta_max.npy'), np.array(train_loss_values))\n",
    "    np.save(os.path.join(dir_root, 'runs/val_error_100_eta_max.npy'), np.array(val_error))\n",
    "    np.save(os.path.join(dir_root, 'runs/val_loss_values_100_eta_max.npy'), np.array(val_loss_values))\n",
    "    np.save(os.path.join(dir_root, 'runs/learning_rates_100_eta_max.npy'), np.array(learning_rates))\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path_100}, skipping training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning rate, training and validation loss and error as a function of the number of weight updates. \n",
    "\n",
    "train_error_100_eta_max = np.load(os.path.join(dir_root, 'runs/train_error_100_eta_max.npy'))\n",
    "train_loss_values_100_eta_max = np.load(os.path.join(dir_root, 'runs/train_loss_values_100_eta_max.npy'))\n",
    "val_error_100_eta_max = np.load(os.path.join(dir_root, 'runs/val_error_100_eta_max.npy'))\n",
    "val_loss_values_100_eta_max = np.load(os.path.join(dir_root, 'runs/val_loss_values_100_eta_max.npy'))\n",
    "learning_rates_100_eta_max = np.load(os.path.join(dir_root, 'runs/learning_rates_100_eta_max.npy'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_error_100_eta_max, label='Train Error')\n",
    "plt.plot(val_error_100_eta_max, label='Validation Error')\n",
    "plt.xlabel('Number of Weight Updates')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error vs Number of Weight Updates')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss_values_100_eta_max, label='Train Loss')\n",
    "plt.plot(val_loss_values_100_eta_max, label='Validation Loss')\n",
    "plt.xlabel('Number of Weight Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs Number of Weight Updates')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(learning_rates_100_eta_max)\n",
    "plt.xlabel('Number of Weight Updates')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate vs Number of Weight Updates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c747df",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "There are heuristics in the optimization process that help pick hyper-parameters in practice. We picked the momentum parameter to be $\\rho = 0.9$ in the previous two parts. We will now show experimentally that if\n",
    "\n",
    "$$\\frac{\\eta_{max}}{1 - \\rho}$$\n",
    "\n",
    "is kept unchanged, the validation error more or less remains the same. You will train the network for 50 epochs and measure the validation error at the end of the 50 epochs for three settings:\n",
    "\n",
    "(i) $\\eta_{max}$ and $\\rho = 0.9$\n",
    "\n",
    "(ii) $\\eta_{max} ← 5\\eta_{max}$ and $\\rho = 0.5$\n",
    "\n",
    "(iii) $\\eta_{max} ← \\eta_{max}$ and $\\rho = 0.5$\n",
    "\n",
    "You will notice that the validation error is about the same for the first two but increases for the third setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60863d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_momentum(\n",
    "    net, optimizer, criterion, train_loader, test_loader, epochs, model_name\n",
    "):\n",
    "    model = net.to(device)\n",
    "    total_step = len(train_loader)\n",
    "    val_error = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 130 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{total_step}], \"\n",
    "                    f\"Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_acc = 100 * correct / total\n",
    "            val_err = 100 - val_acc\n",
    "            val_error.append(val_err)\n",
    "            print(f'Validation Error: {val_err:.2f}%')\n",
    "\n",
    "    return val_error\n",
    "\n",
    "# Test the three settings\n",
    "epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# (i) η_max and ρ = 0.9\n",
    "model_1 = allcnn_t().to(device)\n",
    "optimizer_1 = optim.SGD(\n",
    "    model_1.parameters(), \n",
    "    lr=eta_max,\n",
    "    momentum=0.9, \n",
    "    weight_decay=1e-3, \n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with η_max and ρ = 0.9\")\n",
    "val_error_1 = train_momentum(\n",
    "    model_1, optimizer_1, criterion, trainloader, testloader, epochs, \"setting1\"\n",
    ")\n",
    "\n",
    "# (ii) 5*η_max and ρ = 0.5\n",
    "model_2 = allcnn_t().to(device)\n",
    "optimizer_2 = optim.SGD(\n",
    "    model_2.parameters(), \n",
    "    lr=5*eta_max,\n",
    "    momentum=0.5, \n",
    "    weight_decay=1e-3, \n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with 5*η_max and ρ = 0.5\")\n",
    "val_error_2 = train_momentum(\n",
    "    model_2, optimizer_2, criterion, trainloader, testloader, epochs, \"setting2\"\n",
    ")\n",
    "\n",
    "# (iii) η_max and ρ = 0.5\n",
    "model_3 = allcnn_t().to(device)\n",
    "optimizer_3 = optim.SGD(\n",
    "    model_3.parameters(), \n",
    "    lr=eta_max,\n",
    "    momentum=0.5, \n",
    "    weight_decay=1e-3, \n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with η_max and ρ = 0.5\")\n",
    "val_error_3 = train_momentum(\n",
    "    model_3, optimizer_3, criterion, trainloader, testloader, epochs, \"setting3\"\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val_error_1, label='η_max, ρ=0.9')\n",
    "plt.plot(val_error_2, label='5*η_max, ρ=0.5') \n",
    "plt.plot(val_error_3, label='η_max, ρ=0.5')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Error (%)')\n",
    "plt.title('Validation Error vs Epochs for Different Momentum Settings')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print final validation errors\n",
    "print(\"\\nFinal Validation Errors:\")\n",
    "print(f\"Setting 1 (η_max, ρ=0.9): {val_error_1[-1]:.2f}%\")\n",
    "print(f\"Setting 2 (5*η_max, ρ=0.5): {val_error_2[-1]:.2f}%\")\n",
    "print(f\"Setting 3 (η_max, ρ=0.5): {val_error_3[-1]:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0V11OJs90wGu",
   "metadata": {
    "id": "0V11OJs90wGu"
   },
   "outputs": [],
   "source": [
    "# Shut down if it's google colab\n",
    "# First sleep for a while so that changes to the notebook are saved\n",
    "import time\n",
    "time.sleep(60)\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ese546",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
