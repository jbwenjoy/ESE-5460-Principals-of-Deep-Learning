{
 "cells": [
  {
   "metadata": {
    "id": "9107cce774b4dc37",
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:44.136086Z",
     "start_time": "2024-11-27T18:30:34.170452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys, subprocess, json, argparse\n",
    "from itertools import product\n"
   ],
   "id": "9107cce774b4dc37",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a450805ac6a98a5",
    "outputId": "7e2fa026-f82b-40c7-f5fd-8d9890481bd4",
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:44.244065Z",
     "start_time": "2024-11-27T18:30:44.156611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", device)\n",
    "\n",
    "dir_root = ''\n",
    "# If using google colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    dir_root = '/content/drive/MyDrive/Colab Notebooks/ESE546/hw4'\n",
    "\n",
    "print(\"dir_root:\", dir_root)"
   ],
   "id": "1a450805ac6a98a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "dir_root: \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c599442d8304e9e8",
    "outputId": "8545f3ab-5fa5-4dc0-871f-8ae64b5ad9fa",
    "ExecuteTime": {
     "end_time": "2024-11-27T18:30:47.892471Z",
     "start_time": "2024-11-27T18:30:46.808491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the dataset directory\n",
    "data_dir = os.path.join(dir_root, 'data')\n",
    "print(data_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, 'cifar-10-batches-py')):\n",
    "    download = True\n",
    "    print('Dataset not found, downloading...')\n",
    "else:\n",
    "    download = False\n",
    "    print('Dataset found, not downloading.')\n",
    "\n",
    "# Reading in the dataset\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_dir, train=True,\n",
    "                                        download=download, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_dir, train=False,\n",
    "                                       download=download, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "id": "c599442d8304e9e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "Dataset found, not downloading.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "73a904e76d2d9a97",
    "ExecuteTime": {
     "end_time": "2024-11-27T18:31:31.890284Z",
     "start_time": "2024-11-27T18:31:31.874654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# File allcnn.py provided by Prof. Pratik Chaudhari\n",
    "# at https://gist.github.com/pratikac/68d6d94e4739786798e90691fb1a581b\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, o):\n",
    "        super().__init__()\n",
    "        self.o = o\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, self.o)\n",
    "\n",
    "class allcnn_t(nn.Module):\n",
    "    def __init__(self, c1=96, c2=192):\n",
    "        super().__init__()\n",
    "        d = 0.5\n",
    "\n",
    "        def convbn(ci, co, ksz, s=1, pz=0):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ci, co, ksz, stride=s, padding=pz),\n",
    "                nn.ReLU(True),\n",
    "                nn.BatchNorm2d(co))\n",
    "\n",
    "        self.m = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            convbn(3, c1, 3, 1, 1),\n",
    "            convbn(c1, c1, 3, 1, 1),\n",
    "            convbn(c1, c1, 3, 2, 1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c1, c2, 3, 1, 1),\n",
    "            convbn(c2, c2, 3, 1, 1),\n",
    "            convbn(c2, c2, 3, 2, 1),\n",
    "            nn.Dropout(d),\n",
    "            convbn(c2, c2, 3, 1, 1),\n",
    "            convbn(c2, c2, 3, 1, 1),\n",
    "            convbn(c2, 10, 1, 1),\n",
    "            nn.AvgPool2d(8),\n",
    "            View(10))\n",
    "\n",
    "        print('Num parameters: ', sum([p.numel() for p in self.m.parameters()]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.m(x)\n"
   ],
   "id": "73a904e76d2d9a97",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "d7f37b7c635c4283",
    "ExecuteTime": {
     "end_time": "2024-11-27T18:31:35.965081Z",
     "start_time": "2024-11-27T18:31:35.949444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Weight decay = 1e-3\n",
    "# SGD with Nesterov’s momentum of 0.9\n",
    "# Dropout = 0.5\n",
    "# Learning rate starts with eta_0 = 1e-5,\n",
    "# then eta_tp1 = 1.1 * eta_t, (t <= 100)\n",
    "\n",
    "# First train 100 iters\n",
    "# Record the average training loss of each mini-batch separately and the learning rate that was used for it for about 100 iterations.\n",
    "# Plot the training loss (Y-axis) as a function of the learning rate (X-axis); use a log-scale for the X-axis.\n",
    "\n",
    "# Initialize the logger\n",
    "logger = SummaryWriter(os.path.join(dir_root, 'runs/cnn_experiment'))\n",
    "\n",
    "def train(net, optimizer, criterion, train_loader, test_loader, epochs, model_name, plot):\n",
    "    model = net.to(device)\n",
    "    total_step = len(train_loader)\n",
    "    overall_step = 0\n",
    "    train_loss_values = []\n",
    "    train_error = []\n",
    "    val_loss_values = []\n",
    "    val_error = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        flag = 0\n",
    "        running_loss = 0.0\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            # Move tensors to configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if (i+1) % 1000 == 0:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Learning Rate: {:.3g}, Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, optimizer.param_groups[0]['lr'], loss.item()))\n",
    "            if plot:\n",
    "              info = { ('loss_' + model_name): loss.item() }\n",
    "\n",
    "              for tag, value in info.items():\n",
    "                logger.add_scalar(tag, value, overall_step+1)\n",
    "\n",
    "        # Update learning rate every epoch\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 1.05\n",
    "\n",
    "        train_loss_values.append(running_loss)\n",
    "        train_error.append(100 - 100 * correct / total)\n",
    "\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))\n",
    "        val_error.append(100 - 100 * correct / total)\n",
    "        val_loss_values.append(val_running_loss)\n",
    "    return val_error, val_loss_values, train_error, train_loss_values, learning_rates\n"
   ],
   "id": "d7f37b7c635c4283",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71255eb6cdf29300",
    "outputId": "60b6a7a4-0d29-43ec-c79e-e18abbb07b72",
    "ExecuteTime": {
     "end_time": "2024-11-27T18:32:10.874838Z",
     "start_time": "2024-11-27T18:32:10.843590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Weight decay = 1e-3\n",
    "# SGD with Nesterov’s momentum of 0.9\n",
    "# Dropout = 0.5\n",
    "# Learning rate starts with eta_0 = 1e-5,\n",
    "# then eta_tp1 = 1.1 * eta_t, (t <= 100)\n",
    "\n",
    "model_path_100 = os.path.join(dir_root, 'runs/hw4p3_model_100_epoch.pt')\n",
    "\n",
    "TRAIN_FLAG = True\n",
    "\n",
    "if not os.path.exists(model_path_100) or TRAIN_FLAG:\n",
    "    model = allcnn_t().to(device)\n",
    "    epochs = 100\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "\n",
    "    val_error, val_loss_values, train_error, train_loss_values, learning_rates = train(model, optimizer, criterion, trainloader, testloader, epochs, 'cnn_curve_100', True)\n",
    "\n",
    "    torch.save(model, model_path_100)\n",
    "\n",
    "    np.save(os.path.join(dir_root, 'runs/train_error_100.npy'), np.array(train_error))\n",
    "    np.save(os.path.join(dir_root, 'runs/train_loss_values_100.npy'), np.array(train_loss_values))\n",
    "    np.save(os.path.join(dir_root, 'runs/val_error_100.npy'), np.array(val_error))\n",
    "    np.save(os.path.join(dir_root, 'runs/val_loss_values_100.npy'), np.array(val_loss_values))\n",
    "    np.save(os.path.join(dir_root, 'runs/learning_rates_100.npy'), np.array(learning_rates))\n",
    "else:\n",
    "    print(f\"Model already exists at {model_path_100}, skipping training.\")"
   ],
   "id": "71255eb6cdf29300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at runs/hw4p3_model_100_epoch.pt, skipping training.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the npy files for plotting\n",
    "train_error_100 = np.load(os.path.join(dir_root, 'runs/train_error_100.npy'))\n",
    "train_loss_values_100 = np.load(os.path.join(dir_root, 'runs/train_loss_values_100.npy'))\n",
    "val_error_100 = np.load(os.path.join(dir_root, 'runs/val_error_100.npy'))\n",
    "val_loss_values_100 = np.load(os.path.join(dir_root, 'runs/val_loss_values_100.npy'))\n",
    "learning_rates_100 = np.load(os.path.join(dir_root, 'runs/learning_rates_100.npy'))\n",
    "\n",
    "# Plot the training loss (Y-axis) as a function of the learning rate (X-axis); use a log-scale for the X-axis.\n",
    "plt.figure()\n",
    "plt.plot(train_loss_values_100, learning_rates_100)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Training Loss')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Training Loss vs Learning Rate')\n",
    "plt.show()"
   ],
   "id": "5dce6693780a48c3"
  },
  {
   "metadata": {
    "id": "1f4e3683ebfba8b2"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Then\n",
    "# Use cosine learning rate schedule with a warmup\n",
    "# \\eta(t) = 1e-4 + t / T * eta_max, if t <= T0\n",
    "# \\eta(t) = eta_max * cos(pi / 2 * (t - T0) / (T - T0)) + 1e-6, if T0 < t <= T"
   ],
   "id": "1f4e3683ebfba8b2"
  },
  {
   "cell_type": "code",
   "source": [
    "# Shut down if it's google colab\n",
    "# First sleep for a while so that changes to the notebook are saved\n",
    "import time\n",
    "time.sleep(60)\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()"
   ],
   "metadata": {
    "id": "0V11OJs90wGu"
   },
   "id": "0V11OJs90wGu",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
